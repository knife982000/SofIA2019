{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8wGDUw32HSX8"
   },
   "source": [
    "# Impacto Profundo en el ML\n",
    "Deep Learning es un conjunto de técnicas de *Machine Learing* basadas en lo que se conoce como Redes Neuronales Artificiales. Algunas características del Deep Learning son:\n",
    "* Se basa en Redes Neuronales Artificiales con muchas capas.\n",
    "* Utiliza aprendizaje por gradiente.\n",
    "* Reducen el esfuerzo necesario para extraer características  de los datos originales.\n",
    "* Funciona mejor cuando mayor cantidad de datos hay disponibles.\n",
    "\n",
    "## Aprendizaje con métodos basados en el gradiente\n",
    "El método más común de entrenar redes neuronales son los métodos basados en gradiente. Consideremos el siguiente conjunto de datos generados, donde $x$ es la variable que conocemos e $y$ la que se quiere predecir.\n",
    "\n",
    "$$y=3 * x+(rand-0.5)$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3611,
     "status": "ok",
     "timestamp": 1560361661755,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "1rmd-25lUx0w",
    "outputId": "06ad8b20-7e83-4381-d1d9-5d3118e8affa"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def gen_random_data(mult):\n",
    "    _x = np.linspace(-1, 1, 50)\n",
    "    _error = (np.random.rand(*_x.shape) - .5)\n",
    "    _y = _x * mult + _error\n",
    "    return _x, _y\n",
    "\n",
    "\n",
    "x, y = gen_random_data(3)\n",
    "plt.plot(x, y, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eh_nRPQQVMOB"
   },
   "source": [
    "## Objetivo\n",
    "Considerando la varaible independiente $x$ y la variable dependiente $y$, el objetivo de un regresión lineal es encontrar $w$ y $b$ tal que dada una función de error $E(y, \\hat{y})$ sea mínimo. Es decir:\n",
    "\n",
    "$$\\underset{w,b}{arg\\,min}=E(y,xw+b)$$\n",
    "\n",
    "## Función de error\n",
    "Una función de error utilizada para este tipo de problemas es el error medio cuadrático (_mean squared error_), que se define como:\n",
    "\n",
    "$$MSE(y,\\hat{y})=\\frac{1}{N}\\sum(y-\\hat{y})^{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1366,
     "status": "ok",
     "timestamp": 1560361672613,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "dQLTOW3DVjkF",
    "outputId": "9bca88eb-3eb8-4fa3-b47f-baac4734c400"
   },
   "outputs": [],
   "source": [
    "#Función de predicción\n",
    "def lineal(x, w, b):\n",
    "    return x * w + b\n",
    "\n",
    "\n",
    "plt.plot(x, y, 'ro', x, lineal(x, 3, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1090,
     "status": "ok",
     "timestamp": 1560361680317,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "XCo2rtdRVwey",
    "outputId": "ce615d81-b542-42e4-d1bf-dba66a0febab"
   },
   "outputs": [],
   "source": [
    "#Función de error\n",
    "def mse(y_true, y_pred):\n",
    "    return np.average((y_true - y_pred) ** 2)\n",
    "\n",
    "print('El MSE es {}'.format(mse(y, lineal(x, 3, 0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DMagyJlGWCWP"
   },
   "source": [
    "## Optimización\n",
    "El problema en la regresión lineal es encontrar los parámetros que minimiza el valor de la función de error. A continuación, se presenta un gráfico mostrando el valor de la función de $mse$ para diversos valores de $w$ y $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2144,
     "status": "ok",
     "timestamp": 1560361684290,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "ZnWKFhLTWPmK",
    "outputId": "41f37f91-c113-4bbf-9b65-323fec88630a"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "# Construyendo datos\n",
    "w = np.arange(1, 5, 0.1)\n",
    "b = np.arange(-1, 1, 0.01)\n",
    "w, b = np.meshgrid(w, b)\n",
    "e = np.empty_like(w)\n",
    "for i in range(w.shape[0]):\n",
    "    for j in range(w.shape[1]):\n",
    "        e[i, j] = mse(y, lineal(x, w[i, j], b[i, j]))\n",
    "\n",
    "\n",
    "# Plot the surface.\n",
    "surf = ax.plot_surface(w, b, e, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YyCbPHicWVnK"
   },
   "source": [
    "Obviamente, calculando el error para diversos valores de $w$ y $b$ se puede seleccionar el mínimo. Sin embargo, esto es impracticable cuando existen muchos parámetros o puntos de datos.\n",
    "Por simplicidad, vamos a suponer que se conoce $b=0$ resultando en que $\\hat{y}=xw$, por simplicidad la llamaremos $h(x)$. Entonces, nuestro único problema sería encontrar $w$. En este caso, si graficamos la curva de error obtendríamos lo siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1613,
     "status": "ok",
     "timestamp": 1560361694516,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "rUrxbbHsWUoL",
    "outputId": "a66445bf-a10a-4457-8d7b-7f711e82e957"
   },
   "outputs": [],
   "source": [
    "def exp_error(y, x, ws):\n",
    "    def single_error(w):\n",
    "        return mse(y, lineal(x, w, 0))\n",
    "    _s = np.vectorize(single_error)\n",
    "    return _s(ws)\n",
    "\n",
    "ws = np.linspace(1, 5, 51)\n",
    "plt.plot(ws, exp_error(y, x, ws))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Dk9mzeHWfWp"
   },
   "source": [
    "## Solución\n",
    "Dado que la función de error tiene un solo mínimo, se podrían tomar 2 valores cercanos de manera de conocer en qué dirección es conveniente explorar. La función lineal en realidad es una función que depende no solo de los datos $x$, sino que también del parámetro a aprender $w$, entonces la notaremos como $h(x,w)$. Para conocer la pendiente de la función de error dado el parámetro a conocer debemos hacer:\n",
    "\n",
    "$$pendiente_w(w_{1}, w_{0})=\\frac{MSE(y,h(x,w_{1}))-MSE(y,h(x,w_{0}))}{w_{1}-w_{0}}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2522,
     "status": "ok",
     "timestamp": 1560361702145,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "UBiItVXGWlpJ",
    "outputId": "36de3e3f-00b8-480a-b62c-1f7ff39018ad"
   },
   "outputs": [],
   "source": [
    "errors = exp_error(y, x, ws)\n",
    "pendiente = (errors[10]-errors[20])/(ws[10]-ws[20])\n",
    "correccion_ordenada_origen = -pendiente*ws[10] + errors[10]\n",
    "plt.plot(ws, errors, ws[10:21], lineal(pendiente, ws[10:21], 0)+correccion_ordenada_origen)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1EyIVRgMJsNi"
   },
   "source": [
    "Entonces, se podría inicializar $w$ de forma aleatoria e ir actualizando el valor en contra de la pendiente.\n",
    "```\n",
    "for i in range(ciclos):\n",
    "    pw, pb = pendiente(w, b)\n",
    "    w = w - lr * pw\n",
    "    b = b - lr * pb\n",
    "```\n",
    "\n",
    "`lr` y `ciclos` son lo que se conocen como hiperparámetros del algoritmo y dependiendo su selección será cuan bien y rápido el algoritmo llegue a un resultado. \n",
    "Por ejemplo, si `lr` es muy pequeño el algoritmo tardará mucho en ajustar los parámetros. Sin embargo, si `lr` es muy grande el algoritmo rebotará entre valores de error muy alto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1363,
     "status": "ok",
     "timestamp": 1560361713758,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "Ibl2cP-3WtLU",
    "outputId": "a610b9fc-9679-4c0d-83b0-be4501abf302"
   },
   "outputs": [],
   "source": [
    "def pendiente(y_true, x, w, b, delta=1e-6):\n",
    "    pw = (mse(y_true, lineal(x, w + delta, b))-mse(y_true, lineal(x, w, b))) / delta\n",
    "    pb = (mse(y_true, lineal(x, w, b + delta))-mse(y_true, lineal(x, w, b))) / delta\n",
    "    return pw, pb\n",
    "\n",
    "w = 0 #Podría ser cualquier valor\n",
    "b = 1 \n",
    "ciclos = 100\n",
    "lr = 0.1 ## Valores pequeños y grandes: 0.01 y 10\n",
    "errors = []\n",
    "for i in range(ciclos):\n",
    "    pw, pb = pendiente(y, x, w, b)\n",
    "    errors.append(mse(y, lineal(x, w, b)))\n",
    "    w = w - lr * pw\n",
    "    b = b - lr * pb\n",
    "print('Errores a medida que se actualiza el valor de w')\n",
    "plt.plot(errors)\n",
    "plt.show()\n",
    "print('El w final es {}'.format(w))\n",
    "print('El b final es {}'.format(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b6K6FSedW12d"
   },
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Cuando consideramos la pendiente entre dos puntos muy cercanos, en realidad podemos considerar que la pendiente es la derivada para esa variable en ese punto.\n",
    "\n",
    "$$\\lim_{\\Delta \\rightarrow 0} pendiente_w(w_{0}+\\Delta,w_{0})= \\lim_{\\Delta \\rightarrow 0} \\frac{MSE(y,h(x,w_{0}+\\Delta, b))-MSE(y,h(x,w_{0}, b))}{\\Delta} =\\frac{dMSE(y,h(x,w,b))}{dw}$$\n",
    "\n",
    "$$\\lim_{\\Delta \\rightarrow 0} pendiente_b(b_{0}+\\Delta,b_{0})= \\lim_{\\Delta \\rightarrow 0} \\frac{MSE(y,h(x,w, b_{0}+\\Delta))-MSE(y,h(x,w,b_{0}))}{\\Delta} =\\frac{dMSE(y,h(x,w,b))}{db}$$\n",
    "\n",
    "Cuando generalizamos esto para todas las variables que se desean optimizar, en realidad trabajamos sobre el gradiente:\n",
    "\n",
    "$$\\nabla MSE(w,b)=(\\frac{dMSE(w,b)}{dw},\\frac{dMSE(w,b)}{db})$$\n",
    "\n",
    "Utilizar el gradiente tiene dos ventajas:\n",
    " 1. Reduce la cantidad de computo.\n",
    " 2. Reduce errores de precisión de punto flotante.\n",
    " \n",
    " Con esto, el algoritmo anterior se transforma en:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1292,
     "status": "ok",
     "timestamp": 1560361723275,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "LzuzNh98aar5",
    "outputId": "64960a35-fd3c-48f8-ec29-c404072b1e5d"
   },
   "outputs": [],
   "source": [
    "def gradiente(y_true, x, w, b):\n",
    "    gw = -2 * np.average((y_true -lineal(w, x, b) ) * x)\n",
    "    gb = -2 * np.average((y_true -lineal(w, x, b) ))\n",
    "    return gw, gb\n",
    "  \n",
    "w = np.random.uniform(-50, 50) #genera un flotante aleatorio\n",
    "b = np.random.uniform(-50, 50) #genera un flotante aleatorio\n",
    "print('Valores iniciales. w={} b={}'.format(w, b))\n",
    "ciclos = 100\n",
    "lr = 0.1\n",
    "errors = []\n",
    "for i in range(ciclos):\n",
    "    pw, pb = gradiente(y, x, w, b)\n",
    "    errors.append(mse(y, lineal(x, w, b)))\n",
    "    w = w - lr * pw\n",
    "    b = b - lr * pb\n",
    "print('Errores a medida que se actualiza el valor de w')\n",
    "plt.plot(errors)\n",
    "plt.show()\n",
    "print('El w final es {}'.format(w))\n",
    "print('El b final es {}'.format(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "whFhH7ZRb0Ki"
   },
   "source": [
    "En el caso general,  una instancia puede tener muchas características, por lo tanto $\\bar{x}=(x_1, x_2, ..., x_m)$ y $\\bar{w}=(w_1, w_2, ..., w_m)$ es un vectores y la fórmula de la regresión lineal es:\n",
    "\n",
    "$$f(x)=x_1 * w_1+ x_2 * w_2 + ... + x_m * w_m +b$$\n",
    "\n",
    "Si consideramos el producto interno entre los vectores, se puede expresar como:\n",
    "\n",
    "$$f(x)=\\bar{x} \\cdot \\bar{w} +b$$\n",
    "\n",
    "Para acelerar la computación, se suele calcular sobre muchas instancias, con lo que:\n",
    "\n",
    "$$ X = \\left[\\begin{array}{c}\n",
    "\\bar{x}_1^T\\\\\n",
    "\\bar{x}_2^T\\\\\n",
    "...\\\\\n",
    "\\bar{x}_n^T\\\\\n",
    "\\end{array}\\right] =\n",
    "\\left[\\begin{array}{cccc}\n",
    "x_{1, 1} & x_{1, 2} & ... & x_{1, m}\\\\\n",
    "x_{2, 1} & x_{2, 2} & ... & x_{2, m}\\\\\n",
    "... & ... & ... & ...\\\\\n",
    "x_{n, 1} & x_{n, 2} & ... & x_{n, m}\\\\\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "En este caso, el producto interno sigue funcionando, ya que el vector se comporta como una matriz de $(m, 1)$.\n",
    "\n",
    "# Regresión logística\n",
    "Lo anterior se conoce como regresión lineal, que sirve para cuando se quiere predecir un valor en un continuo. Sin embargo, no funciona muy bien cuando queremos resolver problemas de clasificación. \n",
    "\n",
    "La regresión logística (_Logistic Regression_) es un tipo de regresión cuyo objetivo es determinar la probabilidad de que una instancia pertesca a una clase $y$, dado un conjunto de variables independientes $x_i$ que la definen. En este contexto, las instancias están representadas como un vector de variables independientes $x=\\mathbb{R}^{n}$ y una clase $y=\\{0,1\\}$. Es decir:\n",
    "\n",
    "$$P(y|\\bar{x})=h(\\bar{x})$$\n",
    "\n",
    "En este contexto, la función seleccionada para hacer esta estimación por excelencia es la sigmoide.\n",
    "\n",
    "$$sigmoid(z)=\\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "donde: \n",
    "\n",
    "$$z=\\bar{x} \\cdot \\bar{w}+b$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1460,
     "status": "ok",
     "timestamp": 1560361729341,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "G4pOBLIVdVit",
    "outputId": "567adb65-7e2b-4f4b-f27a-4fe8966934e3"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.linspace(-6, 6, 250)\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "heM1qAVudaiK"
   },
   "source": [
    "Además de que la imagen de esta función está en $(0, 1)$, la derivada de esta función es:\n",
    "$$\\frac{sigmoid(z)}{dz}=sigmoid(z)(1-sigmoid(z))$$\n",
    "Lo que facilitaba su implementación.\n",
    "\n",
    "En este contexto, $z$ es una combinación lineal de las variables $x$.\n",
    "\n",
    "\n",
    "## Función de error\n",
    "Para calcular el error, se utiliza la entropía cruzada entre el valor esperado y el valor obtenido.\n",
    "$$CE(y,\\hat{y})=\\frac{\\sum(-y*log(\\hat{y})-(1-y)*log(1-\\hat{y}))}{N}$$\n",
    "En este contexto, la entropía cruzada se interpreta como la información promedio (en bits) necesaria para determinar el valor de $y$ dado que se conoce el valor de $\\hat{y}$.\n",
    "\n",
    "__Nota__: Por simplicidad, se interpreta el logaritmo como logaritmo natural, pero se puede usar logaritmo en cualquier base, ya que solo afecta en una constante.\n",
    "\n",
    "## Ejemplo\n",
    "Para el ejemplo de regresión logística se utilizará el conjunto de datos de cáncer de pecho provisto. Este conjuntos de datos fue recolectado por investigadores de la Universidad de Wisconsin y provisto por la [UCI](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)). Para acceder al conjunto de datos, no es necesario descargarlo y convertirlo al formato, ya que en encuentra provisto por el módulo de [_sklearn.datasets_](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer) de la librería sickit-learn, que es una librería de _machine learning_ que se utilizará durante el curso.\n",
    "\n",
    "El dataset tiene 569 instancias, con 30 atributos cada una. Las instancias pueden ser clasificadas entre Malignos y Benignos. El dataset está ligeramente desbalanceado, lo que significa que existen más instancias de una clase que de la otra. En particular, 37,25% de las instancias son Malignas y 62,75% son Benignas. La siguiente tabla resume el conjunto de datos:\n",
    "\n",
    "| Propiedad | Valor |\n",
    "| --- | --- |\n",
    "| Clases | 2 |\n",
    "| Ejemplos por clase | 212(M-0), 357(B-1) | \n",
    "| Total de instancias | 569 |\n",
    "| Dimensionalidad | 30|\n",
    "\n",
    "El siguiente código:\n",
    "1. Levanta los datos divididos en `x` (atributos) e `y` (clase).\n",
    "1. Divide los datos en entrenamiento y testing.\n",
    "1. Escala los datos de entrenamiento a valores entre 0 y 1.\n",
    "1. Aplica las correcciones de escalado al conjunto de testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6263,
     "status": "ok",
     "timestamp": 1560361744132,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "8y_858qvdqt4",
    "outputId": "25247bf9-ae25-4238-9f75-b471a434a68a"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "x, y = load_breast_cancer(True)\n",
    "x_train = x[:500,:]\n",
    "y_train = y[:500]\n",
    "x_test = x[500:,:]\n",
    "y_test = y[500:]\n",
    "\n",
    "maxs = np.max(x_train, axis=0)\n",
    "mins = np.min(x_train, axis=0)\n",
    "x_train = (x_train - mins) / (maxs - mins)\n",
    "x_test = (x_test - mins) / (maxs - mins)\n",
    "\n",
    "print('El conjuto de datos tiene {} instancias con {} caracteristicas.'.format(*x.shape))\n",
    "print('Se dividio en {} instancias de entrenamiento y {} de test'.format(x_train.shape[0], x_test.shape[0]))\n",
    "\n",
    "\n",
    "print('Si utilizamos T-SNE para visualizar el conjunto de entrenamiento...')\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "ts_rep = TSNE().fit_transform(x_train)\n",
    "for point, label in zip(ts_rep, y_train):\n",
    "    rep = 'b*' if label == 1 else 'r*'\n",
    "    plt.plot([point[0]], [point[1]], rep)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4aEWKVmfendY"
   },
   "source": [
    "Implementar los gradientes es una tarea compleja y propensa a errores, pero frameworks como [Tensorflow](https://www.tensorflow.org) proveen funcionalidad para derivar los gradientes desde las funciones definidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1108
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8334,
     "status": "ok",
     "timestamp": 1560361765798,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "5EUQltRheua0",
    "outputId": "dac53493-1771-4286-ecaf-df1f7907af74"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "'''Esta función dibuja bonita la matríz de confunsión.\n",
    "'''\n",
    "def show_confusion_matrix(cm, labels):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cm)\n",
    "    plt.title('Matriz de confusión')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticklabels([''] + labels)\n",
    "    ax.set_yticklabels([''] + labels)\n",
    "    plt.xlabel('Predicho')\n",
    "    plt.ylabel('Verdadero')\n",
    "    for i, row in zip(range(len(cm)), cm):\n",
    "        for j, val in zip(range(len(row)), row):\n",
    "            ax.text(i, j, str(val), va='center', ha='center').set_backgroundcolor('white')\n",
    "    plt.show()\n",
    "\n",
    "rng = np.random\n",
    "ciclos = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Placeholder de las entradas\n",
    "X = tf.placeholder(tf.float32, [None, 30])\n",
    "Y = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "W = tf.Variable(rng.randn(30).astype(np.float32), name=\"weight\")\n",
    "b = tf.Variable(rng.randn(), name=\"bias\")\n",
    "\n",
    "# Modelo logístico\n",
    "logreg = tf.add(tf.reduce_sum(tf.matmul(X, tf.expand_dims(W, axis=1)), axis=1), b)\n",
    "logreg = 1.0 / tf.add(1.0, tf.exp(-logreg))\n",
    "# Error de entropía cruzada\n",
    "cost = tf.reduce_mean(-Y * tf.log(logreg) - (1 - Y) * tf.log(1 - logreg))\n",
    "# Gradient descent\n",
    "# minimize() sabe que hay que modificar W y b porque están configuradas como trainable=True por defecto\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializa las variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Comenzar una sessión\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Inicializar\n",
    "    sess.run(init)\n",
    "\n",
    "    y_pred = sess.run(logreg, feed_dict={X: x_test, Y:y_test})\n",
    "    print('Error: {}'.format(sess.run(cost, feed_dict={X: x_test, Y:y_test})))\n",
    "    show_confusion_matrix(confusion_matrix(y_test, y_pred > 0.5), labels=['Maligno', 'Benigno'])\n",
    "    errors = []\n",
    "    errors_test = []\n",
    "    print('Entrenando')\n",
    "    for epoch in range(ciclos):\n",
    "        sess.run(optimizer, feed_dict={X: x_train, Y: y_train})\n",
    "        errors.append(sess.run(cost, feed_dict={X: x_train, Y:y_train}))\n",
    "        errors_test.append(sess.run(cost, feed_dict={X: x_test, Y:y_test}))\n",
    "    print('Error en entrenamiento')\n",
    "    plt.plot(range(ciclos), errors, 'b-', range(ciclos), errors_test, 'r-')\n",
    "    plt.show()\n",
    "    y_pred = sess.run(logreg, feed_dict={X: x_test, Y:y_test})\n",
    "    print('Error: {}'.format(sess.run(cost, feed_dict={X: x_test, Y:y_test})))\n",
    "    show_confusion_matrix(confusion_matrix(y_test, y_pred > 0.5), labels=['Maligno', 'Benigno'])\n",
    "    print('El w final es {}'.format(sess.run(W)))\n",
    "    print('El b final es {}'.format(sess.run(b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kau5QGmXi7oH"
   },
   "source": [
    "## Keras\n",
    "\n",
    "De hecho, este tipo de operaciones es tan común que frameworks de más alto nivel, como [Keras](https://keras.io/) ya lo traen implementado.\n",
    "\n",
    "Es más, elementos como regresiones lineales o logísticas son considerados como neuronas en una red neuronal ([Imagen de Wikipedia](https://commons.wikimedia.org/wiki/File:Artificial_neural_network.png)):\n",
    "![Esquema básido de una neurona](https://upload.wikimedia.org/wikipedia/commons/b/b6/Artificial_neural_network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1226
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5342,
     "status": "ok",
     "timestamp": 1560361906239,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "ckdFDu8aj4am",
    "outputId": "5038eff0-cfa1-4c21-e70a-39e24ec265af"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "#Defino la entrada que tiene forma  (None, 30)\n",
    "i = Input((x_train.shape[1],))\n",
    "#Defino una capa densa que es activation(x*w+b) \n",
    "#* es producto interno\n",
    "#el kernel (w) tiene la forma (30, 1)\n",
    "#el bias (b) tiene la forma (1,)\n",
    "d = Dense(1, activation='sigmoid')(i)\n",
    "model = Model(inputs=i, outputs=d)\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd')\n",
    "\n",
    "#imprimo el modelo\n",
    "model.summary()\n",
    "\n",
    "show_confusion_matrix(confusion_matrix(y_test, model.predict(x_test) > 0.5), labels=['Maligno', 'Benigno'])\n",
    "#Por formato remuevo la última dimensión que es 1\n",
    "print('El w final es {}'.format(K.get_value(model.layers[-1].kernel)[:, 0])) \n",
    "print('El b final es {}'.format(K.get_value(model.layers[-1].bias)))\n",
    "\n",
    "h = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test), verbose=0)\n",
    "plt.plot(h.history['loss'], 'b-', h.history['val_loss'], 'r-')\n",
    "plt.show()\n",
    "\n",
    "show_confusion_matrix(confusion_matrix(y_test, model.predict(x_test) > 0.5), labels=['Maligno', 'Benigno'])\n",
    "#Por formato remuevo la última dimensión que es 1\n",
    "print('El w final es {}'.format(K.get_value(model.layers[-1].kernel)[:, 0]))\n",
    "print('El b final es {}'.format(K.get_value(model.layers[-1].bias)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LgwIeExvobIm"
   },
   "source": [
    "## Problema de OCR de dígitos\n",
    "Para este trabajo utilizaremos el conjunto de datos conocido como [MNIST](http://yann.lecun.com/exdb/mnist/). Este conjunto de datos ya se encuentra dividido entre entrenamiento y testing. El problema consiste en clasificar imágenes de dígitos escritos a mano al dígito correspondiente.\n",
    "\n",
    "| Propiedad | Valor |\n",
    "| --- | --- |\n",
    "| Clases | 10 |\n",
    "| Tamaño de las imagenes | 28 X 28 |\n",
    "| Instancias de entrenemiento | 60.000 |\n",
    "| Instancias de testeo | 10.000 |\n",
    "| Valor mínimo de cada pixel | 0 |\n",
    "| Valor máximo de cada pixel | 255 |\n",
    "\n",
    "A continuación, se carga el dataset y se dibujan los primeros 100 ejemplos del conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10854,
     "status": "ok",
     "timestamp": 1560361921292,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "8yQg45ReocXw",
    "outputId": "389c5531-206f-41de-86e9-e9c965a21840"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print('100 primeros elementos del conjunto de entrenamiento')\n",
    "f = plt.figure(111)\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        ax = f.add_subplot(10, 10, i + j*10 + 1)\n",
    "        ax.set_xticklabels('')\n",
    "        ax.set_yticklabels('')\n",
    "        ax.imshow(x_train[i + j*10, :, :], cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "size = x_train.shape[1]*x_train.shape[2]\n",
    "x_train = x_train.reshape((x_train.shape[0], size)) / 255\n",
    "x_test = x_test.reshape((x_test.shape[0], size)) / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tZKae8Dro3I9"
   },
   "source": [
    "Esto es un problema multiclase, que se podría pensar como una competencia entre 10 clasificadores que consideren cada uno de los $784$ pixeles de la imagen como sus características. Es decir, un clasificador para la clase 0, otro para la clase 1, otro para la clase 2, hasta llegar al 9. A la hora de entrenar, cada regresión logística se entrena por separado. Mientras que cuando se clasifica, la clase seleccionada es el clasificador que obtuvo la máxima probabilidad. \n",
    "\n",
    "Para acelerar el cálculo de los 10 clasificadores, se pueden unir los 10 vectores de peso $\\bar{w}_c$ en una matriz, y los biases $b_c$ en un vector. Entonces:\n",
    "\n",
    "$$W=\\left[\\begin{array}{cccc}\n",
    "\\bar{w}_0 & \\bar{w}_1 & ... & \\bar{w}_9\\\\\n",
    "\\end{array}\\right]=$\\left[\\begin{array}{cccc}\n",
    "w_{1,0} & w_{1,1} & ... & w_{1,9}\\\\\n",
    "w_{2,0} & w_{2,1} & ... & w_{2,9}\\\\\n",
    "...\\\\\n",
    "w_{784,0} & w_{784,1} & ... & w_{784,9}\\\\\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "$$\\bar{b}=(b_0, b_1, ..., b_9)$$\n",
    "\n",
    "Con esta definición, es fácil mostrar que:\n",
    "\n",
    "$$f(X) = X \\cdot W + \\bar{b} =\\left[\\begin{array}{cccc}\n",
    "X \\cdot \\bar{w_0} + b_0 & X \\cdot \\bar{w_1} + b_1 & ... & X \\cdot \\bar{w_9} + b_9 \n",
    "\\end{array}\\right]$$\n",
    "\n",
    "Con lo que cada columna de la matriz resultante, representa la salida de un clasificado para cada instancia.\n",
    "Por lo tanto, en este ejemplo, es necesario transformar las etiquetas a la codificación one-hot con el fin de que el formato de las etiquetas sea compatible con el formato de salida de los clasificadores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 763,
     "status": "ok",
     "timestamp": 1558912827981,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "R8Lunxhm4HOn",
    "outputId": "1a03787d-0b27-40d5-e2ca-cc081f85e218"
   },
   "outputs": [],
   "source": [
    "print('El las etiquetas en el conjunto de entrenamiento tienen la forma {}'.format(y_train.shape))\n",
    "print('Las primeras 10 estiquetas son {}'.format(y_train[:10]))\n",
    "print('Transformadas a categoricas tienen la siguiente forma:')\n",
    "print(to_categorical(y_train[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1323
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19118,
     "status": "ok",
     "timestamp": 1560361957864,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "7vrYzBSipQGc",
    "outputId": "1822aed4-52f6-45db-87e7-a7c33abf22d9"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score as acc\n",
    "\n",
    "def show_confusion_matrix_nl(cm):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cm)\n",
    "    plt.title('Matriz de confusión')\n",
    "    fig.colorbar(cax)\n",
    "    plt.xlabel('Predicho')\n",
    "    plt.ylabel('Verdadero')\n",
    "    plt.show()\n",
    "\n",
    "#Inicializo la entrada como un vector de tamaño size\n",
    "i = Input(shape=(size,)) \n",
    "#Inicializo una capa densa, activación sigmoide y entrada i\n",
    "#Para inicializar la capa densa se usa la API funcional de keras\n",
    "d = Dense(10, activation='sigmoid')(i) \n",
    "#Inicializo el modelo a partir de sus entradas y salidas\n",
    "model = Model(inputs=i, outputs=d)\n",
    "#Compilo el modelo con la función de pedidad y utilizando \n",
    "#Stocastic Gradiant Descent como optimizador (una variante del Gradient Descent)\n",
    "#metrics no es necesario, pero permite usar otra función de error para la validación\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['categorical_accuracy'])\n",
    "#Muestro la esturctura del perceptrón\n",
    "model.summary()\n",
    "#Me quedo con la mayor predicción\n",
    "predict = lambda x: np.argmax(model.predict(x), axis=-1)\n",
    "\n",
    "show_confusion_matrix_nl(confusion_matrix(y_test, predict(x_test)))\n",
    "print('La accuracy antes de entrenar es {}'.format(acc(y_test, predict(x_test))))\n",
    "#Entreno y guardo el historial del errores en h. \n",
    "#verbose: 0: sin salida, 1: salida detallada, 2: salida solo al final del epoch\n",
    "h = model.fit(x_train, to_categorical(y_train), \n",
    "              batch_size=100, epochs=10, \n",
    "              validation_data=(x_test, to_categorical(y_test)), verbose=0)\n",
    "\n",
    "print('Función de pérdidad:')\n",
    "plt.plot(h.history['loss'], 'b-', h.history['val_loss'], 'r-')\n",
    "plt.show()\n",
    "print('Accuracy:')\n",
    "plt.plot(h.history['categorical_accuracy'], 'b-', h.history['val_categorical_accuracy'], 'r-')\n",
    "plt.show()\n",
    "\n",
    "show_confusion_matrix_nl(confusion_matrix(y_test, predict(x_test)))\n",
    "print('La accuracy después de entrenar es {}'.format(acc(y_test, predict(x_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iQJ_ns6ssi6x"
   },
   "source": [
    "## Competencias entre clases\n",
    "Uno de los problemas más grandes que tiene la versión de arriba es que no hay competencia entre las clases. Si agregamos información de que una imagen no puede pertenecer a dos clases, se puede mejorar el clasificador. Para esto, utilizamos una función de activación conocida como softmax:\n",
    "\n",
    "$$Softmax(\\bar{z})_{i}=\\frac{e^{z_{i}}}{\\sum e^{z_{j}}} $$\n",
    "\n",
    "Como resultado de utilizar la función, la suma de las probabilidades para una instancia es 1.\n",
    "Además, se cambia la función de error a la entropía cruzada categórica\n",
    "\n",
    "$$CC(Y, \\hat{Y})=-{\\sum Y \\circ log(\\hat{Y})} $$\n",
    "\n",
    "Donde $y$ es una matriz de la cantidad de instancias por la cantidad de clases. En cada fila tiene un uno para la clase correspondiente a la instancia y ceros en todos los demás elementos. $\\hat{Y}$ es una matriz de las mismas dimensiones, es la salida del clasificador. Y $\\circ$ es el producto elemento a elemento, conocido como producto de Hadamard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1329
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18814,
     "status": "ok",
     "timestamp": 1558913069402,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "HskkYoa_sjJB",
    "outputId": "794b7732-f65a-48ce-d665-abd5baeafe54"
   },
   "outputs": [],
   "source": [
    "#Inicializo la entrada como un vector de tamaño size\n",
    "i = Input(shape=(size,)) \n",
    "#Inicializo una capa densa, activación sigmoide y entrada i\n",
    "#Para inicializar la capa densa se usa la API funcional de keras\n",
    "d = Dense(10, activation='softmax')(i) \n",
    "#Inicializo el modelo a partir de sus entradas y salidas\n",
    "model = Model(inputs=i, outputs=d)\n",
    "#Compilo el modelo con la función de pedidad y utilizando \n",
    "#Stocastic Gradiant Descent como optimizador (una variante del Gradient Descent)\n",
    "#metrics no es necesario, pero permite usar otra función de error para la validación\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['categorical_accuracy'])\n",
    "#Muestro la esturctura del perceptrón\n",
    "model.summary()\n",
    "\n",
    "show_confusion_matrix_nl(confusion_matrix(y_test, predict(x_test)))\n",
    "print('La accuracy antes de entrenar es {}'.format(acc(y_test, predict(x_test))))\n",
    "#Entreno y guardo el historial del errores en h. \n",
    "#verbose: 0: sin salida, 1: salida detallada, 2: salida solo al final del epoch\n",
    "h = model.fit(x_train, to_categorical(y_train), \n",
    "              batch_size=100, epochs=10, \n",
    "              validation_data=(x_test, to_categorical(y_test)), verbose=0)\n",
    "\n",
    "print('Función de pérdidad:')\n",
    "plt.plot(h.history['loss'], 'b-', h.history['val_loss'], 'r-')\n",
    "plt.show()\n",
    "print('Accuracy:')\n",
    "plt.plot(h.history['categorical_accuracy'], 'b-', h.history['val_categorical_accuracy'], 'r-')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "show_confusion_matrix_nl(confusion_matrix(y_test, predict(x_test)))\n",
    "print('La accuracy después de entrenar es {}'.format(acc(y_test, predict(x_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pPDWXa3LxkD-"
   },
   "source": [
    "Hasta aquí, se puede observar que funciones lineales tienen una buena capacidad de predicción. Si vemos los resultados, en el $90\\%$ de los casos la clasificación fue correcta. Como el dataset es balanceado, es decir, tiene la misma cantidad de instancias de cada clase podemos decir que tanto un clasificador aleatorio o un clasificador de clase mayoritaria obtienen, en promedio, clasificarían correctamente el $90\\%$ de los casos.\n",
    "\n",
    "## Red Neuronal Profunda\n",
    "\n",
    "Uno de los tipos redes neuronales profundas consisten en apilar varios de estos clasificadores. Estas se suelen llamar *Deep Feedforward Networks* y su expresión matemática es:\n",
    "\n",
    "$$L1(X)=act_1(X \\cdot W_1 + \\bar{b}_1)$$\n",
    "$$L2(L1(X)=act_2(L1(X) \\cdot W_2 + \\bar{b}_2)$$\n",
    "$$L3(L2(L1(X)))=act_3(L2(L1(X)) \\cdot W_3 + \\bar{b}_3)$$\n",
    "$$...$$\n",
    "$$Ln(...L3(L2(L1(X))...)=act_n(Ln(...(L3((L1(X))...) \\cdot W_n + \\bar{b}_n)$$\n",
    "\n",
    "El concepto es que $L1$ extraiga características lineales de los datos, $L2$ cuadráticas, y así hasta que $L_n$ haga la predicción. $act_i$ es lo que se denomina función de activación, generalmente se suelen usar:\n",
    "* **Sigmoide**: acotada entre $(0,1)$, con su valor medio en $s(0)=0.5$\n",
    "* **Tanh**: Similar a la sigmoide, pero acotada entre $(-1, 1)$\n",
    "* **Lineal o identidad**: una función que retorna el valor de entrada $f(x)=x$\n",
    "* **Relu (Rectified Linear Unit)**: función lineal si $x > 0$, sino es contante en $0$\n",
    "\n",
    "![Red Neuronal](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8b/Neural_network_bottleneck_achitecture.svg/800px-Neural_network_bottleneck_achitecture.svg.png)\n",
    "\n",
    "Fuente: [Wikimedia](https://commons.wikimedia.org/wiki/File:Neural_network_bottleneck_achitecture.svg)\n",
    "\n",
    "\n",
    "**NOTA:** Si bien está probado que una red con 3 capas puede aprender cualquier función, no se sabe como determinar el número necesario de unidades o características que la capa intermedia debe aprender. Además, son difíciles de entrenar con métodos basados en el gradiente, ya que son muy sensibles a los cambios.\n",
    "\n",
    "**NOTA 2:** Las funciones de activación Relu y Lineal son preferidas a para las capas intermedias, ya que tiende facilitar el entrenamiento. Funciones como la sigmoide o tanh, pueden tener gradientes pequeños que se pierden por la limitación en la representación de los números flotantes. Esto se conoce como ***vanishing gradient problem***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1435
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21768,
     "status": "ok",
     "timestamp": 1558913483143,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "eSZI_7aXxkSm",
    "outputId": "72f0ac25-c3da-4c5d-ef42-e82fd2527dac"
   },
   "outputs": [],
   "source": [
    "#Inicializo la entrada como un vector de tamaño size\n",
    "i = Input(shape=(size,)) \n",
    "#Inicializo una capa densa, activación sigmoide y entrada i\n",
    "#Para inicializar la capa densa se usa la API funcional de keras\n",
    "d = Dense(100, activation='relu')(i)\n",
    "d = Dense(100, activation='relu')(d)\n",
    "d = Dense(100, activation='relu')(d)\n",
    "d = Dense(10, activation='softmax')(d) \n",
    "#Inicializo el modelo a partir de sus entradas y salidas\n",
    "model = Model(inputs=i, outputs=d)\n",
    "#Compilo el modelo con la función de pedidad y utilizando \n",
    "#Stocastic Gradiant Descent como optimizador (una variante del Gradient Descent)\n",
    "#metrics no es necesario, pero permite usar otra función de error para la validación\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['categorical_accuracy'])\n",
    "#Muestro la esturctura del perceptrón\n",
    "model.summary()\n",
    "\n",
    "show_confusion_matrix_nl(confusion_matrix(y_test, predict(x_test)))\n",
    "print('La accuracy antes de entrenar es {}'.format(acc(y_test, predict(x_test))))\n",
    "#Entreno y guardo el historial del errores en h. \n",
    "#verbose: 0: sin salida, 1: salida detallada, 2: salida solo al final del epoch\n",
    "h = model.fit(x_train, to_categorical(y_train), \n",
    "              batch_size=100, epochs=10, \n",
    "              validation_data=(x_test, to_categorical(y_test)), verbose=0)\n",
    "\n",
    "print('Función de pérdidad:')\n",
    "plt.plot(h.history['loss'], 'b-', h.history['val_loss'], 'r-')\n",
    "plt.show()\n",
    "print('Accuracy:')\n",
    "plt.plot(h.history['categorical_accuracy'], 'b-', h.history['val_categorical_accuracy'], 'r-')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "show_confusion_matrix_nl(confusion_matrix(y_test, predict(x_test)))\n",
    "print('La accuracy después de entrenar es {}'.format(acc(y_test, predict(x_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xrejJaZu3TBM"
   },
   "source": [
    "Con una red con cerca de 100000 parámetros se llega a una taza de error de menos del $6\\%$ en la clasificación.\n",
    "\n",
    "Hasta aquí hemos visto que:\n",
    "\n",
    "![ML comici](https://imgs.xkcd.com/comics/machine_learning.png)\n",
    "\n",
    "Fuente [xkcd](https://xkcd.com/1838/)\n",
    "\n",
    "## Redes convolucionales\n",
    "\n",
    "Las redes convolucionales, o CNN (Convolutional Neural Network), son un tipo especializado de redes neuronales que han sido aplicado con mucho éxito en problemas en cuales los datos tienen forma de grillas, como son las imágenes. Este éxito es atribuido a que las características de una imagen tienen un fuerte principio de localidad. Por ejemplo, la existencia de un borde puede ser determinada mirando solo los pixeles cercanos al mismo verificando si existe una variación súbita de colores. Se las conocen como redes convolucionales ya que aplican una operación matemática conocida como convolución. La convolución no es más que un operador móvil que se aplica repetidamente sobre los datos de entrada. Este operador está definido por una matriz pequeña, generalmente llamada kernel, que se aplica repetidamente sobre la imagen. Por ejemplo, imaginemos un kernel de 2 X 2.\n",
    "\n",
    "$$K=\\left[\\begin{array}{cc}\n",
    "k_{1,1} & k_{1,2}\\\\\n",
    "k_{2,1} & k_{2,2}\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "y una imagen de n X m:\n",
    "\n",
    "$$I=\\left[\\begin{array}{cc}\n",
    "i_{1,1} & i_{1,2} & ... & i_{1, m}\\\\\n",
    "i_{2,1} & i_{2,2} & ... & i_{2, m}\\\\\n",
    "... & ... & ... & ... \\\\\n",
    "i_{n,1} & i_{n,2} & ... & i_{n, m}\\\\\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "el resultado de aplicar la convolución sería:\n",
    "\n",
    "$$C=\\left[\\begin{array}{cc}\n",
    "c_{1,1} & c_{1,2} & ... & c_{1, m-1}\\\\\n",
    "c_{2,1} & c_{2,2} & ... & c_{2, m-1}\\\\\n",
    "... & ... & ... & ... \\\\\n",
    "c_{n-1,1} & c_{n-1,2} & ... & c_{n-1, m-1}\\\\\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "donde:\n",
    "\n",
    "$$c_{i, j} = i_{i, j} * k_{1,1} + i_{i, j+1} * k_{1,2} + i_{i+1, j} * k_{2,1} + i_{i+1, j+2} * k_{2,2}$$\n",
    "\n",
    "La operación de convolución ha sido usada con mucho éxito en procesamiento de imágenes para detección de bordes, mejoramiento de imágenes, aplicación de blur, etc. Por ejemplo, Kirsch[1] propuso en 1971 una técnica que permite detectar estructuras en las imágenes. Para esto, utiliza distintas [matrices de convolución](https://en.wikipedia.org/wiki/Kirsch_operator). Para ilustrar el la convolución utilizaremos $g^{(1)}$.\n",
    "\n",
    "$$g^{(1)}=\\left[\\begin{array}{cc}\n",
    "5 & 5 & 5 \\\\\n",
    "-3 & 0 & -3 \\\\\n",
    "-3 & -3 & -3\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "[1] Kirsch, R. (1971). \"[Computer determination of the constituent structure of biological images](https://www.sciencedirect.com/science/article/pii/0010480971900346)\". Computers and Biomedical Research. 4: 315–328. doi:10.1016/0010-4809(71)90034-6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14307,
     "status": "ok",
     "timestamp": 1558913640724,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "rVTo356O3Sa-",
    "outputId": "30fc3c4c-ffad-4673-e499-b20adab0d5e6"
   },
   "outputs": [],
   "source": [
    "!pip install tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.layers import Input, Dense, Conv2D, Flatten\n",
    "from keras.models import Model \n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from imageio import imread\n",
    "from tqdm import tqdm\n",
    "import os.path\n",
    "while not os.path.exists('movediza.jpg'):\n",
    "    #Si no está el archivo hay que subirlo. Solo para Google Colab!!\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    for fn in uploaded.keys():\n",
    "        print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1519,
     "status": "ok",
     "timestamp": 1558915109414,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "Kx-sH5DyjMwL",
    "outputId": "a3a67a8a-2dc0-4d96-fad7-f063ff7d343d"
   },
   "outputs": [],
   "source": [
    "#Cargo una imagen.\n",
    "movediza = imread('movediza.jpg')\n",
    "print(movediza.shape)\n",
    "plt.imshow(movediza)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MAgXxbx2lZAz"
   },
   "source": [
    "A continuación, se presenta una implementación en Python de un filtro convolucional para ilustrar su funcionamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16884,
     "status": "ok",
     "timestamp": 1558915280841,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "nafiGAg4jM-D",
    "outputId": "84adf3bf-1bf5-40f5-e459-8b685446a72e"
   },
   "outputs": [],
   "source": [
    "#Cargo la matriz\n",
    "kirsch_matrix = [[5, 5, 5], \n",
    "                [-3, 0, -3],\n",
    "                [-3, -3, -3]]\n",
    "kirsch_matrix = np.asarray(kirsch_matrix, dtype=np.float32) \n",
    "#Inicializo el placeholder como una matriz con garbage\n",
    "movediza_k1 = np.empty((movediza.shape[0]-2, movediza.shape[1]-2, 3))\n",
    "#Convolucion por fila, por columna, \n",
    "for i in tqdm(range(0, movediza.shape[0]-2)): #tqdm muestra la barra de progreso\n",
    "    for j in range(0, movediza.shape[1]-2):\n",
    "        #Trato los canales de forma independiente\n",
    "        for c in range(3): \n",
    "            # El operador * multiplica la matriz elemento a elemento y luego la\n",
    "            # reduzco con una suma\n",
    "            movediza_k1[i, j, c] = np.sum(movediza[i:i+3, j:j+3, c] * kirsch_matrix)\n",
    "# Normalizo la imagen para poder mostrarla\n",
    "movediza_k1 = (movediza_k1 - np.min(movediza_k1))/(np.max(movediza_k1) - np.min(movediza_k1))\n",
    "plt.imshow(movediza_k1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rJNKJRbqll18"
   },
   "source": [
    "### Keras\n",
    "La implementación anterior es lenta debido a que no se encuentra adecuadamente vectorizada y que Python es un lenguaje interpretado. \n",
    "\n",
    "Los frameworks para Deep Learning, incluido Keras, proveen soporte para las capas convolucionales. Estas convoluciones son más genéricas que la convolución del ejemplo. En el ejemplo se considera cada canal (color) por separar, pero en las redes convolucionales también se opera entre los canales de la imagen. Además, la salida de la convolución puede tener una cantidad distinta de canales a la entrada. Los canales de salida se los suele llamar filtros, ya que representan un filtro convolucional particular. \n",
    "En el caso de Keras, el kernel de una convolución tiene la forma de `(alto, ancho, canales_entrada, canales_salida)`.\n",
    "\n",
    "Si queremos replicar el ejemplo anterior con Keras, podemos hacer que cada filtro se corresponda con cada y que los valores en un filtro $i$ para un canal $j$ es $0$ si $i \\neq j$, y el valor correspondiente al filtro Kirsch en otro caso.\n",
    "\n",
    "```\n",
    "kernel = np.zeros((3, 3, 3, 3))\n",
    "for i in range(3):\n",
    "    kernel[:, :, i, i] = kirsch_kernel\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4332,
     "status": "ok",
     "timestamp": 1558915457966,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "EJZ-CZJBjNQO",
    "outputId": "e1d0b9e3-3bda-4051-a251-6d6f56a5dd82"
   },
   "outputs": [],
   "source": [
    "# De entrada tengo una matriz de 3 dimensiones, las primeras dos\n",
    "# son de tamaño variable\n",
    "i = Input(shape=(None, None, 3)) \n",
    "# Agrego una capa convolucional de 2 dimensiones\n",
    "d = Conv2D(3, (3,3), activation='linear', use_bias=False)(i)\n",
    "# Creo el modelo\n",
    "kirsch = Model(inputs=i, outputs=d)\n",
    "kirsch.summary()\n",
    "# Compilo el modelo. No importa el loss ni optimizer.\n",
    "kirsch.compile(loss='categorical_crossentropy', optimizer='sgd')\n",
    "print('Forma de los parámetros de la convolución: {}'.format(kirsch.layers[1].kernel.shape))\n",
    "# Cargo la matrix en un kernel apropiado\n",
    "kirsch_kernel = np.zeros((3, 3, 3, 3))\n",
    "for i in range(3):\n",
    "    kirsch_kernel[:, :, i, i] = kirsch_matrix\n",
    "# Utilizo el backend para cargar el kernel construido.\n",
    "K.set_value(kirsch.layers[1].kernel, kirsch_kernel)\n",
    "# Aplico la convolución.\n",
    "movediza_k2 = kirsch.predict(np.asarray([movediza]))[0]\n",
    "# Normalizo la imagen para poder mostrarla\n",
    "movediza_k2 = (movediza_k2 - np.min(movediza_k2))/(np.max(movediza_k2) - np.min(movediza_k2))\n",
    "plt.imshow(movediza_k2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OiH_IqfH24bb"
   },
   "source": [
    "Dado que las capas convolucionales presentan funciones derivables en todos sus parámetros, es posible calcular su gradiente. Por este motivo, podemos utilizarlas, junto a las funciones de activación, para definir una red neuronal. \n",
    "\n",
    "![Red Convolucional](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Typical_cnn.png/800px-Typical_cnn.png)\n",
    "\n",
    "Fuente: [Wikipedia Convolutional Neural Network](https://en.wikipedia.org/wiki/Convolutional_neural_network)\n",
    "\n",
    "Ejemplos de cómo extraen características pueden observarse en:\n",
    "\n",
    "* [An Interactive Node-Link Visualization of Convolutional Neural Networks](http://scs.ryerson.ca/~aharley/vis/)\n",
    "* [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1505
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29626,
     "status": "ok",
     "timestamp": 1558915620368,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "OpB1HlUx61sW",
    "outputId": "238994fc-94d5-43bf-dc45-5e41784d7a00"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "#x_train.shape = (60000, 28, 28)\n",
    "x_train = np.expand_dims(x_train / 255, axis=-1)\n",
    "x_test = np.expand_dims(x_test / 255, axis=-1)\n",
    "\n",
    "#x_train.shape = (60000, 28, 28, 1)\n",
    "i = Input(shape=(x_train.shape[1], x_train.shape[2], 1))\n",
    "d = Conv2D(5, (5,5), activation='relu')(i)\n",
    "d = Conv2D(5, (5,5), activation='relu')(d)\n",
    "d = Conv2D(5, (5,5), activation='relu')(d)\n",
    "d = Conv2D(10, (5,5), activation='relu')(d)\n",
    "d = Flatten()(d)\n",
    "d = Dense(10, activation='softmax')(d)\n",
    "model = Model(inputs=i, outputs=d)\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['categorical_accuracy'])\n",
    "\n",
    "show_confusion_matrix_nl(confusion_matrix(y_test, predict(x_test)))\n",
    "print('La accuracy antes de entrenar es {}'.format(acc(y_test, predict(x_test))))\n",
    "\n",
    "h = model.fit(x_train, to_categorical(y_train), epochs=10, batch_size=100, \n",
    "              validation_data=(x_test, to_categorical(y_test)), verbose=0)\n",
    "\n",
    "print('Función de pérdidad:')\n",
    "plt.plot(h.history['loss'], 'b-', h.history['val_loss'], 'r-')\n",
    "plt.show()\n",
    "print('Accuracy:')\n",
    "plt.plot(h.history['categorical_accuracy'], 'b-', h.history['val_categorical_accuracy'], 'r-')\n",
    "plt.show()\n",
    "\n",
    "show_confusion_matrix_nl(confusion_matrix(y_test, predict(x_test)))\n",
    "print('La accuracy después de entrenar es {}'.format(acc(y_test, predict(x_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RZaNrfzK4Hw5"
   },
   "source": [
    "De la prueba, se puede observar dos cosas importentes:\n",
    "1. Las redes convolucionales tienen menos parámetros que una totalmente conectada.\n",
    "1. Obtuvo mejores resultados.\n",
    "\n",
    "Tener menos parámetros es una ventaja por partida doble. Primero, al haber menos parámetros es más fácil realizar la computación ya que se requiere menos memoria RAM/RAM de GPU para almacenar la red. Segundo, al haber menos parámetros, estos tienen menos posibilidades de especializarse en características particulares del conjunto de entrenamiento. Por lo que se requieren menos datos para entrenar la red y esta generaliza mejor a datos nuevos.\n",
    "\n",
    "Por otro lado, también se pueden mejorar los resultados cambiando otros hiper-parámetros, como la forma de optimizar. Por ejemplo, Nadam que incluye en la fórmula de actualización de los pesos elementos como cuanto varían los gradientes o si la variación es sostenida en el tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1505
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36219,
     "status": "ok",
     "timestamp": 1558915718027,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "WPQjRvDS5Trj",
    "outputId": "fe10b8ce-94ac-4bee-8a79-95ad01adc903"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = np.expand_dims(x_train / 255, axis=-1)\n",
    "x_test = np.expand_dims(x_test / 255, axis=-1)\n",
    "\n",
    "\n",
    "i = Input(shape=(x_train.shape[1], x_train.shape[2], 1))\n",
    "d = Conv2D(5, (5,5), activation='relu')(i)\n",
    "d = Conv2D(5, (5,5), activation='relu')(d)\n",
    "d = Conv2D(5, (5,5), activation='relu')(d)\n",
    "d = Conv2D(10, (5,5), activation='relu')(d)\n",
    "d = Flatten()(d)\n",
    "d = Dense(10, activation='softmax')(d)\n",
    "model = Model(inputs=i, outputs=d)\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['categorical_accuracy'])\n",
    "\n",
    "show_confusion_matrix_nl(confusion_matrix(y_test, predict(x_test)))\n",
    "print('La accuracy antes de entrenar es {}'.format(acc(y_test, predict(x_test))))\n",
    "\n",
    "h = model.fit(x_train, to_categorical(y_train), epochs=10, batch_size=100, \n",
    "              validation_data=(x_test, to_categorical(y_test)), verbose=0)\n",
    "\n",
    "print('Función de pérdidad:')\n",
    "plt.plot(h.history['loss'], 'b-', h.history['val_loss'], 'r-')\n",
    "plt.show()\n",
    "print('Accuracy:')\n",
    "plt.plot(h.history['categorical_accuracy'], 'b-', h.history['val_categorical_accuracy'], 'r-')\n",
    "plt.show()\n",
    "\n",
    "show_confusion_matrix_nl(confusion_matrix(y_test, predict(x_test)))\n",
    "print('La accuracy después de entrenar es {}'.format(acc(y_test, predict(x_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j-zcd4LeDisW"
   },
   "source": [
    "# Transfer learning\n",
    "\n",
    "\n",
    "Trasnfer learning es otra manera de utilizar las técnicas de Deep Learning. Se utiliza en casos donde los datos de entrenamiento son escasos, pero se tiene modelos entrenados para tareas similares. Para ejemplificar, utilizaremos el dataset conocido como [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "\n",
    "| Propiedad | Valor |\n",
    "| --- | --- |\n",
    "| Clases | 10 |\n",
    "| Tamaño de las imágenes | 32 X 32  |\n",
    "| Canales de las imágenes | 3 (RGB)  |\n",
    "| Instancias de entrenamiento | 50.000 |\n",
    "| Instancias de testeo | 10.000 |\n",
    "| Valor mínimo de cada pixel | 0 |\n",
    "| Valor máximo de cada pixel | 255 |\n",
    "\n",
    "El dataset contiene imágenes en color de 32 X 32 pixeles divididas en 10 clases:\n",
    "1. Avión\n",
    "1. Auto\t\t\t\t\t\t\t\t\t\t\n",
    "1. Pájaro\t\t\t\t\t\t\t\t\t\n",
    "1. Gato\t\t\t\t\t\t\t\n",
    "1. Venado\t\t\t\t\t\t\t\t\t\t\n",
    "1. Perro\t\t\t\t\t\t\n",
    "1. Rana\t\t\t\t\t\t\t\t\t\n",
    "1. Caballo\t\t\t\t\t\t\t\t\t\t\n",
    "1. Barco\t\t\t\t\t\t\t\n",
    "1. Camión\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26134,
     "status": "ok",
     "timestamp": 1558915846277,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "NLY8QKQ2DjxJ",
    "outputId": "5b694ecd-38c3-43d9-d308-cf845966acc0"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print('100 primeros elementos del conjunto de entrenamiento')\n",
    "f = plt.figure(111)\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        ax = f.add_subplot(10, 10, i + j*10 + 1)\n",
    "        ax.set_xticklabels('')\n",
    "        ax.set_yticklabels('')\n",
    "        ax.imshow(x_train[i + j*10, :, :], cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mAgyLVeRTqpk"
   },
   "source": [
    "Si utilizamos una CNN con una arquitectura similar a del ejemplo anterior, obtenemos un accuracy del $52\\%$ aproximadamente.\n",
    "\n",
    "**IMPORTANTE:** por cuestiones de tiempo de entrenamiento, se usa la misma arquitectura utilizada en el ejemplo de MNIST, pero se pueden lograr mejores resultados. En el [ejemplo de Keras](https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py) la red puede alcanzar un accuracy del $79\\%$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1505
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 42425,
     "status": "ok",
     "timestamp": 1558915904394,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "i2kNPhoNTrAR",
    "outputId": "ac2ca385-71da-403c-c416-29e99417b73f"
   },
   "outputs": [],
   "source": [
    "from keras.layers import GaussianNoise\n",
    "i = Input(shape=(32, 32, 3))\n",
    "d = Conv2D(5, (5,5), activation='relu')(i)\n",
    "d = Conv2D(5, (5,5), activation='relu')(d)\n",
    "d = Conv2D(5, (5,5), activation='relu')(d)\n",
    "d = Conv2D(10, (5,5), activation='relu')(d)\n",
    "d = Flatten()(d)\n",
    "d = Dense(10, activation='softmax')(d)\n",
    "model = Model(inputs=i, outputs=d)\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['categorical_accuracy'])\n",
    "\n",
    "show_confusion_matrix_nl(confusion_matrix(y_test, predict(x_test)))\n",
    "print('La accuracy antes de entrenar es {}'.format(acc(y_test, predict(x_test))))\n",
    "\n",
    "h = model.fit(x_train, to_categorical(y_train), epochs=10, batch_size=100, \n",
    "              validation_data=(x_test, to_categorical(y_test)), verbose=0)\n",
    "\n",
    "\n",
    "print('Función de pérdidad:')\n",
    "plt.plot(h.history['loss'], 'b-', h.history['val_loss'], 'r-')\n",
    "plt.show()\n",
    "print('Accuracy:')\n",
    "plt.plot(h.history['categorical_accuracy'], 'b-', h.history['val_categorical_accuracy'], 'r-')\n",
    "plt.show()\n",
    "\n",
    "show_confusion_matrix_nl(confusion_matrix(y_test, predict(x_test)))\n",
    "print('La accuracy después de entrenar es {}'.format(acc(y_test, predict(x_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lVbrY6s2xNwC"
   },
   "source": [
    "Supongamos que tenemos solo una porción de datos para entrenar, por ejemplo 2000 imágenes (200 por cada clases). ¿Sería posible entrenar la red neuronal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EzHri-8nxQ34"
   },
   "outputs": [],
   "source": [
    "sample_per_class = 200\n",
    "\n",
    "x_small = np.empty((sample_per_class * 10, 32, 32, 3))\n",
    "y_small = np.empty((sample_per_class * 10,))\n",
    "\n",
    "\n",
    "counter = [0] * 10\n",
    "\n",
    "i = 0\n",
    "for x, y in zip(x_train, y_train):\n",
    "    if counter[y[0]] == sample_per_class:\n",
    "      continue\n",
    "    counter[y[0]] += 1\n",
    "    x_small[i, :, :, :] = x\n",
    "    y_small[i] = y\n",
    "    i += 1\n",
    "    if i == sample_per_class * 10: \n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1505
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10656,
     "status": "ok",
     "timestamp": 1558915978786,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "lNMvJDTRTrKz",
    "outputId": "f9d84e70-83c5-4197-f3b5-1569a7bf2545"
   },
   "outputs": [],
   "source": [
    "i = Input(shape=(32, 32, 3))\n",
    "d = Conv2D(5, (5,5), activation='relu')(i)\n",
    "d = Conv2D(5, (5,5), activation='relu')(d)\n",
    "d = Conv2D(5, (5,5), activation='relu')(d)\n",
    "d = Conv2D(10, (5,5), activation='relu')(d)\n",
    "d = Flatten()(d)\n",
    "d = Dense(10, activation='softmax')(d)\n",
    "model = Model(inputs=i, outputs=d)\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['categorical_accuracy'])\n",
    "\n",
    "show_confusion_matrix_nl(confusion_matrix(y_test, predict(x_test)))\n",
    "print('La accuracy antes de entrenar es {}'.format(acc(y_test, predict(x_test))))\n",
    "\n",
    "h = model.fit(x_small, to_categorical(y_small), epochs=10, batch_size=100, \n",
    "              validation_data=(x_test, to_categorical(y_test)), verbose=0)\n",
    "\n",
    "\n",
    "print('Función de pérdidad:')\n",
    "plt.plot(h.history['loss'], 'b-', h.history['val_loss'], 'r-')\n",
    "plt.show()\n",
    "print('Accuracy:')\n",
    "plt.plot(h.history['categorical_accuracy'], 'b-', h.history['val_categorical_accuracy'], 'r-')\n",
    "plt.show()\n",
    "\n",
    "show_confusion_matrix_nl(confusion_matrix(y_test, predict(x_test)))\n",
    "print('La accuracy después de entrenar es {}'.format(acc(y_test, predict(x_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ngLH-s-4XDO"
   },
   "source": [
    "En el gráfico de accuracy podemos ver que la red aprende muy bien a identificar los ejemplos de entrenamiento. Llega a un accuracy del $40\\%$, pero cuando hacemos la evaluación con el conjunto de test, el valor es del $30\\%$. Este fenómeno se conoce como *overfitting* y es un problema importante cuando se usa este tipo de técnicas con pocos datos.\n",
    "\n",
    "Para este tipo de problemas se utiliza el *transfer learning*. Para esto, se debe considerad alguna red neuronal arbitraría entrenada para clasificar imágenes con un dataset grande. Hay muchas disponibles públicamente. Keras provee varias [redes preentrenadas](https://keras.io/applications/) con el dataset de [ImageNet](http://www.image-net.org/), más de 14 millones de imágenes con 1000 clases. Por ser una arquictura simple, podemos tomar VGG16 que tiene más de **138 millones de parámetros**. A continuación, se puede observar la arquitectura de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 985
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9384,
     "status": "ok",
     "timestamp": 1558916027079,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "vtxAuvqNY2Hh",
    "outputId": "2e953d39-a7c9-4104-ccbe-c34f1e04ccce"
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "model = VGG16(include_top=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gl7mnpebxJcq"
   },
   "source": [
    "Si consideramos que las capas ocultas aprenden las características de las imágenes, podemos separar la red en dos partes:\n",
    "\n",
    "1. Desde la capa de `Input` hasta la capa `block5_pool` como un extractor de características.\n",
    "2. Las capas `fc1` y `fc2` como un clasificador. \n",
    "\n",
    "Si nos quedamos con la primera parte podemos tener un extractor de características para imágenes genéricas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 844
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2418,
     "status": "ok",
     "timestamp": 1558916086703,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "Lp4wZ6IB6vgf",
    "outputId": "ce98cbf6-1d03-4e30-e919-755020b16458"
   },
   "outputs": [],
   "source": [
    "#El modelo es pesado y no queremos que se rompa por falta de memoria en la GPU\n",
    "del model \n",
    "#Ahora si, sin el tope!!\n",
    "model = VGG16(include_top=False)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dG_UEoVH6MVq"
   },
   "source": [
    "Por comparación vamos a crear 2 dataset nuevos:\n",
    "\n",
    "\n",
    "1. **x_small_t** y **x_test_t**: dataset small transformado con el modelo VGG16.\n",
    "2. **x_small_f** y **x_test_f**: dataset small con forma cambiada para que cada pixel de la imagen sea un valor en un vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4676,
     "status": "ok",
     "timestamp": 1558916168995,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "QRxoKuzTN9W8",
    "outputId": "8a1caec9-4884-461e-c14a-ae3e532353f4"
   },
   "outputs": [],
   "source": [
    "#Dataset de transfer learning\n",
    "x_small_t = model.predict(x_small)\n",
    "#Esto hace las veces de flatten\n",
    "x_small_t = np.reshape(x_small_t, (x_small.shape[0], 512))\n",
    "print('Forma del dataset transformado con VGG16 {}'.format(x_small_t.shape))\n",
    "#Test set\n",
    "x_test_t = model.predict(x_test)\n",
    "x_test_t = np.reshape(x_test_t, (x_test.shape[0], 512))\n",
    "\n",
    "\n",
    "#Dataset de imagenes\n",
    "x_small_f = np.reshape(x_small, (x_small.shape[0], 32 * 32 * 3))\n",
    "print('Forma del dataset original {}'.format(x_small_f.shape))\n",
    "x_test_f = np.reshape(x_test, (x_test.shape[0], 32 * 32 * 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-7vbVxKUDURA"
   },
   "source": [
    "Podemos probar los dos tipos de características con una regresión logística:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 71126,
     "status": "ok",
     "timestamp": 1558916351235,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "b0ABXVTNPTNH",
    "outputId": "c9297b2d-7222-4321-adfe-16cc522a1133"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#Los parámetros son para evitar warnings, estandades hasta la versión 0.22\n",
    "mt = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
    "mf = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
    "\n",
    "print('Entrenando Trasnfer')\n",
    "mt.fit(x_small_t, y_small)\n",
    "print('Entrenando Full')\n",
    "mf.fit(x_small_f, y_small)\n",
    "\n",
    "print('Accuracy: {}'.format(acc(y_test, mt.predict(x_test_t))))\n",
    "print('Accuracy: {}'.format(acc(y_test, mf.predict(x_test_f))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klEfdkmN1IBk"
   },
   "source": [
    "Podemos observar que las características transferidas tienen una mejor performance que usar los pixeles de forma cruda.\n",
    "\n",
    "## Fine Tuning\n",
    "\n",
    "Otro uso de las redes preentrenadas para extraer características es incorporarlas en otras redes neuronales para acelerar su entrenamiento. Por ejemplo, en el siguiente caso se utiliza la VGG16 como una capa inicial en una red neuronal. Para que esto funcione, es necesario que las modificaciones en los pesos más sutil que cuando se entrena una red de cero, ya que se supone que la mayoría de los pesos ya están cerca de un valor óptimo. En consecuencia, podemos cambiar el **learning rate** del optimizador, en este caso **Stocastic Gradiant Descent**, de $0.01$ a $0.001$, es decir un orden de magnitud menor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1593
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 133181,
     "status": "ok",
     "timestamp": 1558916602607,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "iv-WCpBlFBcT",
    "outputId": "302f32b9-f5a1-45eb-c599-72f466822db3"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "i = Input((32, 32, 3))\n",
    "model = VGG16(include_top=False)(i)\n",
    "\n",
    "d = Flatten()(model)\n",
    "d = Dense(512, activation='relu')(d)\n",
    "d = Dense(10, activation='softmax')(d)\n",
    "model = Model(inputs=i, outputs=d)\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer= \\\n",
    "              SGD(lr=1e-3, momentum=0.0, decay=0.0, nesterov=False), \\\n",
    "              metrics=['categorical_accuracy'])#1e-4:ok y 30 epocs\n",
    "\n",
    "show_confusion_matrix_nl(confusion_matrix(y_test, predict(x_test)))\n",
    "print('La accuracy antes de entrenar es {}'.format(acc(y_test, predict(x_test))))\n",
    "\n",
    "h = model.fit(x_train, to_categorical(y_train), epochs=4, batch_size=100, \n",
    "              validation_data=(x_test, to_categorical(y_test)), verbose=1)\n",
    "\n",
    "\n",
    "print('Función de pérdidad:')\n",
    "plt.plot(h.history['loss'], 'b-', h.history['val_loss'], 'r-')\n",
    "plt.show()\n",
    "print('Accuracy:')\n",
    "plt.plot(h.history['categorical_accuracy'], 'b-', h.history['val_categorical_accuracy'], 'r-')\n",
    "plt.show()\n",
    "\n",
    "show_confusion_matrix_nl(confusion_matrix(y_test, predict(x_test)))\n",
    "print('La accuracy después de entrenar es {}'.format(acc(y_test, predict(x_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KVtEB1aq6Cmu"
   },
   "source": [
    "Como se puede observar, rápidamente se llega a una accuracy mayor al $0.73$ en validación. Se podría argumentar que es por la arquitectura y no los pesos preentreandos. Sin embargo, si usamos la misma arquitectura sin los pesos preentreandos no se llega a los mismos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1600
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 131012,
     "status": "ok",
     "timestamp": 1558916757091,
     "user": {
      "displayName": "Juan Manuel Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBnxRpk0M4p1fNfSoVnN-Ac0gOsRE8R8pXfAgl1SQo=s64",
      "userId": "04214447791825695719"
     },
     "user_tz": 180
    },
    "id": "PDeahW0TsjOV",
    "outputId": "3be18ca1-5106-48cb-e1fa-efaa2feb4d16"
   },
   "outputs": [],
   "source": [
    "i = Input((32, 32, 3))\n",
    "model = VGG16(include_top=False, weights=None)(i)\n",
    "\n",
    "d = Flatten()(model)\n",
    "d = Dense(512, activation='relu')(d)\n",
    "d = Dense(10, activation='softmax')(d)\n",
    "model = Model(inputs=i, outputs=d)\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', \\\n",
    "              optimizer=SGD(lr=1e-3, momentum=0.0, decay=0.0, nesterov=False),\\\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "show_confusion_matrix_nl(confusion_matrix(y_test, predict(x_test)))\n",
    "print('La accuracy antes de entrenar es {}'.format(acc(y_test, predict(x_test))))\n",
    "\n",
    "h = model.fit(x_train, to_categorical(y_train), epochs=4, batch_size=100, \n",
    "              validation_data=(x_test, to_categorical(y_test)), verbose=1)\n",
    "\n",
    "\n",
    "print('Función de pérdidad:')\n",
    "plt.plot(h.history['loss'], 'b-', h.history['val_loss'], 'r-')\n",
    "plt.show()\n",
    "print('Accuracy:')\n",
    "plt.plot(h.history['categorical_accuracy'], 'b-', h.history['val_categorical_accuracy'], 'r-')\n",
    "plt.show()\n",
    "\n",
    "show_confusion_matrix_nl(confusion_matrix(y_test, predict(x_test)))\n",
    "print('La accuracy después de entrenar es {}'.format(acc(y_test, predict(x_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2fUSv3RWQss0"
   },
   "source": [
    "## Conclusiones\n",
    "\n",
    "![That's not all](https://www.ccbcfamily.org/wp-content/uploads/2018/12/7175_BOB_Spring2019_FINAL.jpg)\n",
    "\n",
    "Esta charla solo cubrió una pequeña parte de todas las aplicaciones de las redes neuronales y el Deep Learning. Queda mucho por ver, como:\n",
    "\n",
    "\n",
    "* **Redes Neuronales recurrentes**: para secuencias temporales, por ejemplo, texto.\n",
    "* **Embeddings**: para características latentes, por ejemplo, palabras (Word2vec, FastText) o recomendación.\n",
    "* **Capas de ruido**: para disminuir el *overfitting*. Ejemplos: Dropout, Ruido Gausiano.\n",
    "* **EarlyStoping y reducción del learning rate**: para aprender mejor durante el entrenamiento.\n",
    "* **Data augmentation**: para incrementar artificialmente la cantidad de datos de entrenamiento.\n",
    "* **Autoencoders**: para realizar ingeniería de características y data augmentation.\n",
    "* **GAN**: para generar nuevas instancias de datos a partir de ejemplos previos.\n",
    "* **...**\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SofiaV2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
