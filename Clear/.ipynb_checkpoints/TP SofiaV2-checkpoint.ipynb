{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8wGDUw32HSX8"
   },
   "source": [
    "# Impacto Profundo en el ML\n",
    "Deep Learning es un conjunto de técnicas de *Machine Learing* basadas en lo que se conoce como Redes Neuronales Artificiales. Algunas características del Deep Learning son:\n",
    "* Se basa en Redes Neuronales Artificiales con muchas capas.\n",
    "* Utiliza aprendizaje por gradiente.\n",
    "* Reducen el esfuerzo necesario para extraer características  de los datos originales.\n",
    "* Funciona mejor cuando mayor cantidad de datos hay disponibles.\n",
    "\n",
    "## Aprendizaje con métodos basados en el gradiente\n",
    "El método más común de entrenar redes neuronales son los métodos basados en gradiente. Consideremos el siguiente conjunto de datos generados, donde $x$ es la variable que conocemos e $y$ la que se quiere predecir.\n",
    "\n",
    "$$y=3 * x+(rand-0.5)$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rmd-25lUx0w"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def gen_random_data(mult):\n",
    "    _x = np.linspace(-1, 1, 50)\n",
    "    _error = (np.random.rand(*_x.shape) - .5)\n",
    "    #TODO: Complete el valor de _y\n",
    "    return _x, _y\n",
    "\n",
    "\n",
    "x, y = gen_random_data(3)\n",
    "#Utilice el método plot para crear un figura de X e Y. 'ro' permite mostrar los \n",
    "#puntos en rojo.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eh_nRPQQVMOB"
   },
   "source": [
    "## Objetivo\n",
    "Considerando la varaible independiente $x$ y la variable dependiente $y$, el objetivo de un regresión lineal es encontrar $w$ y $b$ tal que dada una función de error $E(y, \\hat{y})$ sea mínimo. Es decir:\n",
    "\n",
    "$$\\underset{w,b}{arg\\,min}=E(y,xw+b)$$\n",
    "\n",
    "## Función de error\n",
    "Una función de error utilizada para este tipo de problemas es el error medio cuadrático (_mean squared error_), que se define como:\n",
    "\n",
    "$$MSE(y,\\hat{y})=\\frac{1}{N}\\sum(y-\\hat{y})^{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dQLTOW3DVjkF"
   },
   "outputs": [],
   "source": [
    "#Función de predicción\n",
    "def lineal(x, w, b):\n",
    "    return None #TODO: Implemente la función lineal\n",
    "  \n",
    "\n",
    "plt.plot(x, y, 'ro', x, lineal(x, 3, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XCo2rtdRVwey"
   },
   "outputs": [],
   "source": [
    "#Función de error\n",
    "def mse(y_true, y_pred):\n",
    "    return None #TODO: Implemente el error cuadrado medio. Utilice la función np.average\n",
    "\n",
    "print('El MSE es {}'.format(mse(y, lineal(x, 3, 0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DMagyJlGWCWP"
   },
   "source": [
    "## Optimización\n",
    "El problema en la regresión lineal es encontrar los parámetros que minimiza el valor de la función de error. A continuación, se presenta un gráfico mostrando el valor de la función de $mse$ para diversos valores de $w$ y $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZnWKFhLTWPmK"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "# Construyendo datos\n",
    "w = np.arange(1, 5, 0.1)\n",
    "b = np.arange(-1, 1, 0.01)\n",
    "w, b = np.meshgrid(w, b)\n",
    "e = np.empty_like(w)\n",
    "#TODO: Complete la matriz e, tal que en cada celda i, j\n",
    "#Contenga el error cuadrado medio de utilizar los parámetros\n",
    "#w[i,j] y b[i,j] en el predictro lineal\n",
    "\n",
    "\n",
    "# Plot the surface.\n",
    "surf = ax.plot_surface(w, b, e, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YyCbPHicWVnK"
   },
   "source": [
    "Obviamente, calculando el error para diversos valores de $w$ y $b$ se puede seleccionar el mínimo. Sin embargo, esto es impracticable cuando existen muchos parámetros o puntos de datos.\n",
    "Por simplicidad, vamos a suponer que se conoce $b=0$ resultando en que $\\hat{y}=xw$, por simplicidad la llamaremos $h(x)$. Entonces, nuestro único problema sería encontrar $w$. En este caso, si graficamos la curva de error obtendríamos lo siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rUrxbbHsWUoL"
   },
   "outputs": [],
   "source": [
    "def exp_error(y, x, ws):\n",
    "    def single_error(w):\n",
    "        return mse(y, lineal(x, w, 0))\n",
    "    _s = np.vectorize(single_error)\n",
    "    return _s(ws)\n",
    "\n",
    "ws = np.linspace(1, 5, 51)\n",
    "plt.plot(ws, exp_error(y, x, ws))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Dk9mzeHWfWp"
   },
   "source": [
    "## Solución\n",
    "Dado que la función de error tiene un solo mínimo, se podrían tomar 2 valores cercanos de manera de conocer en qué dirección es conveniente explorar. La función lineal en realidad es una función que depende no solo de los datos $x$, sino que también del parámetro a aprender $w$, entonces la notaremos como $h(x,w)$. Para conocer la pendiente de la función de error dado el parámetro a conocer debemos hacer:\n",
    "\n",
    "$$pendiente_w(w_{1}, w_{0})=\\frac{MSE(y,h(x,w_{1}))-MSE(y,h(x,w_{0}))}{w_{1}-w_{0}}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UBiItVXGWlpJ"
   },
   "outputs": [],
   "source": [
    "errors = exp_error(y, x, ws)\n",
    "pendiente = (errors[10]-errors[20])/(ws[10]-ws[20])\n",
    "correccion_ordenada_origen = -pendiente*ws[10] + errors[10]\n",
    "plt.plot(ws, errors, ws[10:21], lineal(pendiente, ws[10:21], 0)+correccion_ordenada_origen)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1EyIVRgMJsNi"
   },
   "source": [
    "Entonces, se podría inicializar $w$ de forma aleatoria e ir actualizando el valor en contra de la pendiente.\n",
    "```\n",
    "for i in range(ciclos):\n",
    "    pw, pb = pendiente(w, b)\n",
    "    w = w - lr * pw\n",
    "    b = b - lr * pb\n",
    "```\n",
    "\n",
    "`lr` y `ciclos` son lo que se conocen como hiperparámetros del algoritmo y dependiendo su selección será cuan bien y rápido el algoritmo llegue a un resultado. \n",
    "Por ejemplo, si `lr` es muy pequeño el algoritmo tardará mucho en ajustar los parámetros. Sin embargo, si `lr` es muy grande el algoritmo rebotará entre valores de error muy alto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ibl2cP-3WtLU"
   },
   "outputs": [],
   "source": [
    "def pendiente(y_true, x, w, b, delta=1e-6):\n",
    "    pw = 0#TODO: Calcule la pendiente del error al variar entre w+delta y w\n",
    "    pb = 0#TODO: Calcule la pendiente del error al variar entre b+delta y b\n",
    "    return pw, pb\n",
    "\n",
    "w = 0 #Podría ser cualquier valor\n",
    "b = 1 \n",
    "ciclos = 100\n",
    "lr = 0.1 ## Valores pequeños y grandes: 0.01 y 10\n",
    "errors = []\n",
    "for i in range(ciclos):\n",
    "    pw, pb = pendiente(y, x, w, b)\n",
    "    errors.append(mse(y, lineal(x, w, b)))\n",
    "    w = w - lr * pw\n",
    "    b = b - lr * pb\n",
    "print('Errores a medida que se actualiza el valor de w')\n",
    "plt.plot(errors)\n",
    "plt.show()\n",
    "print('El w final es {}'.format(w))\n",
    "print('El b final es {}'.format(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b6K6FSedW12d"
   },
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Cuando consideramos la pendiente entre dos puntos muy cercanos, en realidad podemos considerar que la pendiente es la derivada para esa variable en ese punto.\n",
    "\n",
    "$$\\lim_{\\Delta \\rightarrow 0} pendiente_w(w_{0}+\\Delta,w_{0})= \\lim_{\\Delta \\rightarrow 0} \\frac{MSE(y,h(x,w_{0}+\\Delta, b))-MSE(y,h(x,w_{0}, b))}{\\Delta} =\\frac{dMSE(y,h(x,w,b))}{dw}$$\n",
    "\n",
    "$$\\lim_{\\Delta \\rightarrow 0} pendiente_b(b_{0}+\\Delta,b_{0})= \\lim_{\\Delta \\rightarrow 0} \\frac{MSE(y,h(x,w, b_{0}+\\Delta))-MSE(y,h(x,w,b_{0}))}{\\Delta} =\\frac{dMSE(y,h(x,w,b))}{db}$$\n",
    "\n",
    "Cuando generalizamos esto para todas las variables que se desean optimizar, en realidad trabajamos sobre el gradiente:\n",
    "\n",
    "$$\\nabla MSE(w,b)=(\\frac{dMSE(w,b)}{dw},\\frac{dMSE(w,b)}{db})$$\n",
    "\n",
    "Utilizar el gradiente tiene dos ventajas:\n",
    " 1. Reduce la cantidad de computo.\n",
    " 2. Reduce errores de precisión de punto flotante.\n",
    " \n",
    " Con esto, el algoritmo anterior se transforma en:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LzuzNh98aar5"
   },
   "outputs": [],
   "source": [
    "def gradiente(y_true, x, w, b):\n",
    "    gw = -2 * np.average((y_true -lineal(w, x, b) ) * x)\n",
    "    gb = -2 * np.average((y_true -lineal(w, x, b) ))\n",
    "    return gw, gb\n",
    "  \n",
    "w = np.random.uniform(-50, 50) #genera un flotante aleatorio\n",
    "b = np.random.uniform(-50, 50) #genera un flotante aleatorio\n",
    "print('Valores iniciales. w={} b={}'.format(w, b))\n",
    "ciclos = 100\n",
    "lr = 0.1\n",
    "errors = []\n",
    "for i in range(ciclos):\n",
    "    pw, pb = gradiente(y, x, w, b)\n",
    "    errors.append(mse(y, lineal(x, w, b)))\n",
    "    #TODO: Actualice los valores de w y b\n",
    "print('Errores a medida que se actualiza el valor de w')\n",
    "plt.plot(errors)\n",
    "plt.show()\n",
    "print('El w final es {}'.format(w))\n",
    "print('El b final es {}'.format(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "whFhH7ZRb0Ki"
   },
   "source": [
    "En el caso general,  una instancia puede tener muchas características, por lo tanto $\\bar{x}=(x_1, x_2, ..., x_m)$ y $\\bar{w}=(w_1, w_2, ..., w_m)$ es un vectores y la fórmula de la regresión lineal es:\n",
    "\n",
    "$$f(x)=x_1 * w_1+ x_2 * w_2 + ... + x_m * w_m +b$$\n",
    "\n",
    "Si consideramos el producto interno entre los vectores, se puede expresar como:\n",
    "\n",
    "$$f(x)=\\bar{x} \\cdot \\bar{w} +b$$\n",
    "\n",
    "Para acelerar la computación, se suele calcular sobre muchas instancias, con lo que:\n",
    "\n",
    "$$ X = \\left[\\begin{array}{c}\n",
    "\\bar{x}_1^T\\\\\n",
    "\\bar{x}_2^T\\\\\n",
    "...\\\\\n",
    "\\bar{x}_n^T\\\\\n",
    "\\end{array}\\right] =\n",
    "\\left[\\begin{array}{cccc}\n",
    "x_{1, 1} & x_{1, 2} & ... & x_{1, m}\\\\\n",
    "x_{2, 1} & x_{2, 2} & ... & x_{2, m}\\\\\n",
    "... & ... & ... & ...\\\\\n",
    "x_{n, 1} & x_{n, 2} & ... & x_{n, m}\\\\\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "En este caso, el producto interno sigue funcionando, ya que el vector se comporta como una matriz de $(m, 1)$.\n",
    "\n",
    "# Regresión logística\n",
    "Lo anterior se conoce como regresión lineal, que sirve para cuando se quiere predecir un valor en un continuo. Sin embargo, no funciona muy bien cuando queremos resolver problemas de clasificación. \n",
    "\n",
    "La regresión logística (_Logistic Regression_) es un tipo de regresión cuyo objetivo es determinar la probabilidad de que una instancia pertesca a una clase $y$, dado un conjunto de variables independientes $x_i$ que la definen. En este contexto, las instancias están representadas como un vector de variables independientes $x=\\mathbb{R}^{n}$ y una clase $y=\\{0,1\\}$. Es decir:\n",
    "\n",
    "$$P(y|\\bar{x})=h(\\bar{x})$$\n",
    "\n",
    "En este contexto, la función seleccionada para hacer esta estimación por excelencia es la sigmoide.\n",
    "\n",
    "$$sigmoid(z)=\\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "donde: \n",
    "\n",
    "$$z=\\bar{x} \\cdot \\bar{w}+b$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G4pOBLIVdVit"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return x#TODO: Implemente la función sigmoide\n",
    "\n",
    "x = np.linspace(-6, 6, 250)\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "heM1qAVudaiK"
   },
   "source": [
    "Además de que la imagen de esta función está en $(0, 1)$, la derivada de esta función es:\n",
    "$$\\frac{sigmoid(z)}{dz}=sigmoid(z)(1-sigmoid(z))$$\n",
    "Lo que facilitaba su implementación.\n",
    "\n",
    "En este contexto, $z$ es una combinación lineal de las variables $x$.\n",
    "\n",
    "\n",
    "## Función de error\n",
    "Para calcular el error, se utiliza la entropía cruzada entre el valor esperado y el valor obtenido.\n",
    "$$CE(y,\\hat{y})=\\frac{\\sum(-y*log(\\hat{y})-(1-y)*log(1-\\hat{y}))}{N}$$\n",
    "En este contexto, la entropía cruzada se interpreta como la información promedio (en bits) necesaria para determinar el valor de $y$ dado que se conoce el valor de $\\hat{y}$.\n",
    "\n",
    "__Nota__: Por simplicidad, se interpreta el logaritmo como logaritmo natural, pero se puede usar logaritmo en cualquier base, ya que solo afecta en una constante.\n",
    "\n",
    "## Ejemplo\n",
    "Para el ejemplo de regresión logística se utilizará el conjunto de datos de cáncer de pecho provisto. Este conjuntos de datos fue recolectado por investigadores de la Universidad de Wisconsin y provisto por la [UCI](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)). Para acceder al conjunto de datos, no es necesario descargarlo y convertirlo al formato, ya que en encuentra provisto por el módulo de [_sklearn.datasets_](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer) de la librería sickit-learn, que es una librería de _machine learning_ que se utilizará durante el curso.\n",
    "\n",
    "El dataset tiene 569 instancias, con 30 atributos cada una. Las instancias pueden ser clasificadas entre Malignos y Benignos. El dataset está ligeramente desbalanceado, lo que significa que existen más instancias de una clase que de la otra. En particular, 37,25% de las instancias son Malignas y 62,75% son Benignas. La siguiente tabla resume el conjunto de datos:\n",
    "\n",
    "| Propiedad | Valor |\n",
    "| --- | --- |\n",
    "| Clases | 2 |\n",
    "| Ejemplos por clase | 212(M-0), 357(B-1) | \n",
    "| Total de instancias | 569 |\n",
    "| Dimensionalidad | 30|\n",
    "\n",
    "El siguiente código:\n",
    "1. Levanta los datos divididos en `x` (atributos) e `y` (clase).\n",
    "1. Divide los datos en entrenamiento y testing.\n",
    "1. Escala los datos de entrenamiento a valores entre 0 y 1.\n",
    "1. Aplica las correcciones de escalado al conjunto de testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8y_858qvdqt4"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "x, y = load_breast_cancer(True)\n",
    "x_train = x[:500,:]\n",
    "y_train = y[:500]\n",
    "x_test = x[500:,:]\n",
    "y_test = y[500:]\n",
    "\n",
    "maxs = np.max(x_train, axis=0)\n",
    "mins = np.min(x_train, axis=0)\n",
    "x_train = (x_train - mins) / (maxs - mins)\n",
    "x_test = (x_test - mins) / (maxs - mins)\n",
    "\n",
    "print('El conjuto de datos tiene {} instancias con {} caracteristicas.'.format(*x.shape))\n",
    "print('Se dividio en {} instancias de entrenamiento y {} de test'.format(x_train.shape[0], x_test.shape[0]))\n",
    "\n",
    "\n",
    "print('Si utilizamos T-SNE para visualizar el conjunto de entrenamiento...')\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "ts_rep = TSNE().fit_transform(x_train)\n",
    "for point, label in zip(ts_rep, y_train):\n",
    "    rep = 'b*' if label == 1 else 'r*'\n",
    "    plt.plot([point[0]], [point[1]], rep)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4aEWKVmfendY"
   },
   "source": [
    "Implementar los gradientes es una tarea compleja y propensa a errores, pero frameworks como [Tensorflow](https://www.tensorflow.org) proveen funcionalidad para derivar los gradientes desde las funciones definidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5EUQltRheua0"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "'''Esta función dibuja bonita la matríz de confunsión.\n",
    "'''\n",
    "def show_confusion_matrix(cm, labels):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cm)\n",
    "    plt.title('Matriz de confusión')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticklabels([''] + labels)\n",
    "    ax.set_yticklabels([''] + labels)\n",
    "    plt.xlabel('Predicho')\n",
    "    plt.ylabel('Verdadero')\n",
    "    for i, row in zip(range(len(cm)), cm):\n",
    "        for j, val in zip(range(len(row)), row):\n",
    "            ax.text(i, j, str(val), va='center', ha='center').set_backgroundcolor('white')\n",
    "    plt.show()\n",
    "\n",
    "rng = np.random\n",
    "ciclos = 1000\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Placeholder de las entradas\n",
    "X = tf.placeholder(tf.float32, [None, 30])\n",
    "Y = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "W = tf.Variable(rng.randn(30).astype(np.float32), name=\"weight\")\n",
    "b = tf.Variable(rng.randn(), name=\"bias\")\n",
    "\n",
    "# Modelo logístico\n",
    "lineal = tf.add(tf.reduce_sum(tf.matmul(X, tf.expand_dims(W, axis=1)), axis=1), b)\n",
    "logreg = 1.0 / tf.add(1.0, tf.exp(-lineal))\n",
    "# Error de entropía cruzada\n",
    "cost = 0#TODO: Immplemente la entropia cruzada tf.reduce_mean y tf.log\n",
    "# Gradient descent\n",
    "# minimize() sabe que hay que modificar W y b porque están configuradas como trainable=True por defecto\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializa las variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Comenzar una sessión\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Inicializar\n",
    "    sess.run(init)\n",
    "\n",
    "    y_pred = sess.run(logreg, feed_dict={X: x_test, Y:y_test})\n",
    "    print('Error: {}'.format(sess.run(cost, feed_dict={X: x_test, Y:y_test})))\n",
    "    show_confusion_matrix(confusion_matrix(y_test, y_pred > 0.5), labels=['Maligno', 'Benigno'])\n",
    "    errors = []\n",
    "    errors_test = []\n",
    "    print('Entrenando')\n",
    "    for epoch in range(ciclos):\n",
    "        sess.run(optimizer, feed_dict={X: x_train, Y: y_train})\n",
    "        errors.append(sess.run(cost, feed_dict={X: x_train, Y:y_train}))\n",
    "        errors_test.append(sess.run(cost, feed_dict={X: x_test, Y:y_test}))\n",
    "    print('Error en entrenamiento')\n",
    "    plt.plot(range(ciclos), errors, 'b-', range(ciclos), errors_test, 'r-')\n",
    "    plt.show()\n",
    "    y_pred = sess.run(logreg, feed_dict={X: x_test, Y:y_test})\n",
    "    print('Error: {}'.format(sess.run(cost, feed_dict={X: x_test, Y:y_test})))\n",
    "    show_confusion_matrix(confusion_matrix(y_test, y_pred > 0.5), labels=['Maligno', 'Benigno'])\n",
    "    print('El w final es {}'.format(sess.run(W)))\n",
    "    print('El b final es {}'.format(sess.run(b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kau5QGmXi7oH"
   },
   "source": [
    "## Keras\n",
    "\n",
    "De hecho, este tipo de operaciones es tan común que frameworks de más alto nivel, como [Keras](https://keras.io/) ya lo traen implementado.\n",
    "\n",
    "Es más, elementos como regresiones lineales o logísticas son considerados como neuronas en una red neuronal ([Imagen de Wikipedia](https://commons.wikimedia.org/wiki/File:Artificial_neural_network.png)):\n",
    "![Esquema básido de una neurona](https://upload.wikimedia.org/wikipedia/commons/b/b6/Artificial_neural_network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ckdFDu8aj4am"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "#Defino la entrada que tiene forma  (None, 30)\n",
    "i = Input((x_train.shape[1],))\n",
    "#Defino una capa densa que es activation(x*w+b) \n",
    "#* es producto interno\n",
    "#el kernel (w) tiene la forma (30, 1)\n",
    "#el bias (b) tiene la forma (1,)\n",
    "d = Dense(1, activation='sigmoid')(i)\n",
    "model = Model(inputs=i, outputs=d)\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd')\n",
    "\n",
    "#imprimo el modelo\n",
    "model.summary()\n",
    "\n",
    "show_confusion_matrix(confusion_matrix(y_test, model.predict(x_test) > 0.5), labels=['Maligno', 'Benigno'])\n",
    "#Por formato remuevo la última dimensión que es 1\n",
    "print('El w final es {}'.format(K.get_value(model.layers[-1].kernel)[:, 0])) \n",
    "print('El b final es {}'.format(K.get_value(model.layers[-1].bias)))\n",
    "\n",
    "h = model.fit(x_train, y_train, epochs=100, validation_data=(x_test, y_test), verbose=0)\n",
    "plt.plot(h.history['loss'], 'b-', h.history['val_loss'], 'r-')\n",
    "plt.show()\n",
    "\n",
    "show_confusion_matrix(confusion_matrix(y_test, model.predict(x_test) > 0.5), labels=['Maligno', 'Benigno'])\n",
    "#Por formato remuevo la última dimensión que es 1\n",
    "print('El w final es {}'.format(K.get_value(model.layers[-1].kernel)[:, 0]))\n",
    "print('El b final es {}'.format(K.get_value(model.layers[-1].bias)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LgwIeExvobIm"
   },
   "source": [
    "## Problema de OCR de dígitos\n",
    "Para este trabajo utilizaremos el conjunto de datos conocido como [MNIST](http://yann.lecun.com/exdb/mnist/). Este conjunto de datos ya se encuentra dividido entre entrenamiento y testing. El problema consiste en clasificar imágenes de dígitos escritos a mano al dígito correspondiente.\n",
    "\n",
    "| Propiedad | Valor |\n",
    "| --- | --- |\n",
    "| Clases | 10 |\n",
    "| Tamaño de las imagenes | 28 X 28 |\n",
    "| Instancias de entrenemiento | 60.000 |\n",
    "| Instancias de testeo | 10.000 |\n",
    "| Valor mínimo de cada pixel | 0 |\n",
    "| Valor máximo de cada pixel | 255 |\n",
    "\n",
    "A continuación, se carga el dataset y se dibujan los primeros 100 ejemplos del conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8yQg45ReocXw"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print('100 primeros elementos del conjunto de entrenamiento')\n",
    "f = plt.figure(111)\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        ax = f.add_subplot(10, 10, i + j*10 + 1)\n",
    "        ax.set_xticklabels('')\n",
    "        ax.set_yticklabels('')\n",
    "        ax.imshow(x_train[i + j*10, :, :], cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "size = x_train.shape[1]*x_train.shape[2]\n",
    "x_train = x_train.reshape((x_train.shape[0], size)) / 255\n",
    "x_test = x_test.reshape((x_test.shape[0], size)) / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tZKae8Dro3I9"
   },
   "source": [
    "Esto es un problema multiclase, que se podría pensar como una competencia entre 10 clasificadores que consideren cada uno de los $784$ pixeles de la imagen como sus características. Es decir, un clasificador para la clase 0, otro para la clase 1, otro para la clase 2, hasta llegar al 9. A la hora de entrenar, cada regresión logística se entrena por separado. Mientras que cuando se clasifica, la clase seleccionada es el clasificador que obtuvo la máxima probabilidad. \n",
    "\n",
    "Para acelerar el cálculo de los 10 clasificadores, se pueden unir los 10 vectores de peso $\\bar{w}_c$ en una matriz, y los biases $b_c$ en un vector. Entonces:\n",
    "\n",
    "$$W=\\left[\\begin{array}{cccc}\n",
    "\\bar{w}_0 & \\bar{w}_1 & ... & \\bar{w}_9\\\\\n",
    "\\end{array}\\right]=$\\left[\\begin{array}{cccc}\n",
    "w_{1,0} & w_{1,1} & ... & w_{1,9}\\\\\n",
    "w_{2,0} & w_{2,1} & ... & w_{2,9}\\\\\n",
    "...\\\\\n",
    "w_{784,0} & w_{784,1} & ... & w_{784,9}\\\\\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "$$\\bar{b}=(b_0, b_1, ..., b_9)$$\n",
    "\n",
    "Con esta definición, es fácil mostrar que:\n",
    "\n",
    "$$f(X) = X \\cdot W + \\bar{b} =\\left[\\begin{array}{cccc}\n",
    "X \\cdot \\bar{w_0} + b_0 & X \\cdot \\bar{w_1} + b_1 & ... & X \\cdot \\bar{w_9} + b_9 \n",
    "\\end{array}\\right]$$\n",
    "\n",
    "Con lo que cada columna de la matriz resultante, representa la salida de un clasificado para cada instancia.\n",
    "Por lo tanto, en este ejemplo, es necesario transformar las etiquetas a la codificación one-hot con el fin de que el formato de las etiquetas sea compatible con el formato de salida de los clasificadores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R8Lunxhm4HOn"
   },
   "outputs": [],
   "source": [
    "print('El las etiquetas en el conjunto de entrenamiento tienen la forma {}'.format(y_train.shape))\n",
    "print('Las primeras 10 estiquetas son {}'.format(y_train[:10]))\n",
    "print('Transformadas a categoricas tienen la siguiente forma:')\n",
    "print(to_categorical(y_train[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7vrYzBSipQGc"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score as acc\n",
    "\n",
    "def show_confusion_matrix_nl(cm):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cm)\n",
    "    plt.title('Matriz de confusión')\n",
    "    fig.colorbar(cax)\n",
    "    plt.xlabel('Predicho')\n",
    "    plt.ylabel('Verdadero')\n",
    "    plt.show()\n",
    "\n",
    "#Inicializo la entrada como un vector de tamaño size\n",
    "i = Input(shape=(size,)) \n",
    "#Inicializo una capa densa, activación sigmoide y entrada i\n",
    "#Para inicializar la capa densa se usa la API funcional de keras\n",
    "d = Dense(10, activation='sigmoid')(i) \n",
    "#Inicializo el modelo a partir de sus entradas y salidas\n",
    "model = Model(inputs=i, outputs=d)\n",
    "#Compilo el modelo con la función de pedidad y utilizando \n",
    "#Stocastic Gradiant Descent como optimizador (una variante del Gradient Descent)\n",
    "#metrics no es necesario, pero permite usar otra función de error para la validación\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['categorical_accuracy'])\n",
    "#Muestro la esturctura del perceptrón\n",
    "model.summary()\n",
    "#Me quedo con la mayor predicción\n",
    "predict = lambda x: np.argmax(model.predict(x), axis=-1)\n",
    "\n",
    "show_confusion_matrix_nl(confusion_matrix(y_test, predict(x_test)))\n",
    "print('La accuracy antes de entrenar es {}'.format(acc(y_test, predict(x_test))))\n",
    "#Entreno y guardo el historial del errores en h. \n",
    "#verbose: 0: sin salida, 1: salida detallada, 2: salida solo al final del epoch\n",
    "h = model.fit(x_train, to_categorical(y_train), \n",
    "              batch_size=100, epochs=10, \n",
    "              validation_data=(x_test, to_categorical(y_test)), verbose=0)\n",
    "\n",
    "print('Función de pérdidad:')\n",
    "plt.plot(h.history['loss'], 'b-', h.history['val_loss'], 'r-')\n",
    "plt.show()\n",
    "print('Accuracy:')\n",
    "plt.plot(h.history['categorical_accuracy'], 'b-', h.history['val_categorical_accuracy'], 'r-')\n",
    "plt.show()\n",
    "\n",
    "show_confusion_matrix_nl(confusion_matrix(y_test, predict(x_test)))\n",
    "print('La accuracy después de entrenar es {}'.format(acc(y_test, predict(x_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iQJ_ns6ssi6x"
   },
   "source": [
    "## Competencias entre clases\n",
    "Uno de los problemas más grandes que tiene la versión de arriba es que no hay competencia entre las clases. Si agregamos información de que una imagen no puede pertenecer a dos clases, se puede mejorar el clasificador. Para esto, utilizamos una función de activación conocida como softmax:\n",
    "\n",
    "$$Softmax(\\bar{z})_{i}=\\frac{e^{z_{i}}}{\\sum e^{z_{j}}} $$\n",
    "\n",
    "Como resultado de utilizar la función, la suma de las probabilidades para una instancia es 1.\n",
    "Además, se cambia la función de error a la entropía cruzada categórica\n",
    "\n",
    "$$CC(Y, \\hat{Y})=-{\\sum Y \\circ log(\\hat{Y})} $$\n",
    "\n",
    "Donde $y$ es una matriz de la cantidad de instancias por la cantidad de clases. En cada fila tiene un uno para la clase correspondiente a la instancia y ceros en todos los demás elementos. $\\hat{Y}$ es una matriz de las mismas dimensiones, es la salida del clasificador. Y $\\circ$ es el producto elemento a elemento, conocido como producto de Hadamard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HskkYoa_sjJB"
   },
   "outputs": [],
   "source": [
    "#Inicializo la entrada como un vector de tamaño size\n",
    "i = Input(shape=(size,)) \n",
    "#Inicializo una capa densa, activación sigmoide y entrada i\n",
    "#Para inicializar la capa densa se usa la API funcional de keras\n",
    "d = Dense(10, activation='softmax')(i) \n",
    "#Inicializo el modelo a partir de sus entradas y salidas\n",
    "model = Model(inputs=i, outputs=d)\n",
    "#Compilo el modelo con la función de pedidad y utilizando \n",
    "#Stocastic Gradiant Descent como optimizador (una variante del Gradient Descent)\n",
    "#metrics no es necesario, pero permite usar otra función de error para la validación\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['categorical_accuracy'])\n",
    "#Muestro la esturctura del perceptrón\n",
    "model.summary()\n",
    "\n",
    "show_confusion_matrix_nl(confusion_matrix(y_test, predict(x_test)))\n",
    "print('La accuracy antes de entrenar es {}'.format(acc(y_test, predict(x_test))))\n",
    "#Entreno y guardo el historial del errores en h. \n",
    "#verbose: 0: sin salida, 1: salida detallada, 2: salida solo al final del epoch\n",
    "h = model.fit(x_train, to_categorical(y_train), \n",
    "              batch_size=100, epochs=10, \n",
    "              validation_data=(x_test, to_categorical(y_test)), verbose=0)\n",
    "\n",
    "print('Función de pérdidad:')\n",
    "plt.plot(h.history['loss'], 'b-', h.history['val_loss'], 'r-')\n",
    "plt.show()\n",
    "print('Accuracy:')\n",
    "plt.plot(h.history['categorical_accuracy'], 'b-', h.history['val_categorical_accuracy'], 'r-')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "show_confusion_matrix_nl(confusion_matrix(y_test, predict(x_test)))\n",
    "print('La accuracy después de entrenar es {}'.format(acc(y_test, predict(x_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pPDWXa3LxkD-"
   },
   "source": [
    "Hasta aquí, se puede observar que funciones lineales tienen una buena capacidad de predicción. Si vemos los resultados, en el $90\\%$ de los casos la clasificación fue correcta. Como el dataset es balanceado, es decir, tiene la misma cantidad de instancias de cada clase podemos decir que tanto un clasificador aleatorio o un clasificador de clase mayoritaria obtienen, en promedio, clasificarían correctamente el $90\\%$ de los casos.\n",
    "\n",
    "## Red Neuronal Profunda\n",
    "\n",
    "Uno de los tipos redes neuronales profundas consisten en apilar varios de estos clasificadores. Estas se suelen llamar *Deep Feedforward Networks* y su expresión matemática es:\n",
    "\n",
    "$$L1(X)=act_1(X \\cdot W_1 + \\bar{b}_1)$$\n",
    "$$L2(L1(X)=act_2(L1(X) \\cdot W_2 + \\bar{b}_2)$$\n",
    "$$L3(L2(L1(X)))=act_3(L2(L1(X)) \\cdot W_3 + \\bar{b}_3)$$\n",
    "$$...$$\n",
    "$$Ln(...L3(L2(L1(X))...)=act_n(Ln(...(L3((L1(X))...) \\cdot W_n + \\bar{b}_n)$$\n",
    "\n",
    "El concepto es que $L1$ extraiga características lineales de los datos, $L2$ cuadráticas, y así hasta que $L_n$ haga la predicción. $act_i$ es lo que se denomina función de activación, generalmente se suelen usar:\n",
    "* **Sigmoide**: acotada entre $(0,1)$, con su valor medio en $s(0)=0.5$\n",
    "* **Tanh**: Similar a la sigmoide, pero acotada entre $(-1, 1)$\n",
    "* **Lineal o identidad**: una función que retorna el valor de entrada $f(x)=x$\n",
    "* **Relu (Rectified Linear Unit)**: función lineal si $x > 0$, sino es contante en $0$\n",
    "\n",
    "![Red Neuronal](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8b/Neural_network_bottleneck_achitecture.svg/800px-Neural_network_bottleneck_achitecture.svg.png)\n",
    "\n",
    "Fuente: [Wikimedia](https://commons.wikimedia.org/wiki/File:Neural_network_bottleneck_achitecture.svg)\n",
    "\n",
    "\n",
    "**NOTA:** Si bien está probado que una red con 3 capas puede aprender cualquier función, no se sabe como determinar el número necesario de unidades o características que la capa intermedia debe aprender. Además, son difíciles de entrenar con métodos basados en el gradiente, ya que son muy sensibles a los cambios.\n",
    "\n",
    "**NOTA 2:** Las funciones de activación Relu y Lineal son preferidas a para las capas intermedias, ya que tiende facilitar el entrenamiento. Funciones como la sigmoide o tanh, pueden tener gradientes pequeños que se pierden por la limitación en la representación de los números flotantes. Esto se conoce como ***vanishing gradient problem***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eSZI_7aXxkSm"
   },
   "outputs": [],
   "source": [
    "#Inicializo la entrada como un vector de tamaño size\n",
    "i = Input(shape=(size,)) \n",
    "#Inicializo una capa densa, activación sigmoide y entrada i\n",
    "#Para inicializar la capa densa se usa la API funcional de keras\n",
    "d = Dense(100, activation='relu')(i)\n",
    "\n",
    "#TODO: Apile 2 capas densas de 100 neuronas y activación relu en la variable d\n",
    "\n",
    "d = Dense(10, activation='softmax')(d) \n",
    "#Inicializo el modelo a partir de sus entradas y salidas\n",
    "model = Model(inputs=i, outputs=d)\n",
    "#Compilo el modelo con la función de pedidad y utilizando \n",
    "#Stocastic Gradiant Descent como optimizador (una variante del Gradient Descent)\n",
    "#metrics no es necesario, pero permite usar otra función de error para la validación\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['categorical_accuracy'])\n",
    "#Muestro la esturctura del perceptrón\n",
    "model.summary()\n",
    "\n",
    "show_confusion_matrix_nl(confusion_matrix(y_test, predict(x_test)))\n",
    "print('La accuracy antes de entrenar es {}'.format(acc(y_test, predict(x_test))))\n",
    "#Entreno y guardo el historial del errores en h. \n",
    "#verbose: 0: sin salida, 1: salida detallada, 2: salida solo al final del epoch\n",
    "h = model.fit(x_train, to_categorical(y_train), \n",
    "              batch_size=100, epochs=10, \n",
    "              validation_data=(x_test, to_categorical(y_test)), verbose=0)\n",
    "\n",
    "print('Función de pérdidad:')\n",
    "plt.plot(h.history['loss'], 'b-', h.history['val_loss'], 'r-')\n",
    "plt.show()\n",
    "print('Accuracy:')\n",
    "plt.plot(h.history['categorical_accuracy'], 'b-', h.history['val_categorical_accuracy'], 'r-')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "show_confusion_matrix_nl(confusion_matrix(y_test, predict(x_test)))\n",
    "print('La accuracy después de entrenar es {}'.format(acc(y_test, predict(x_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xrejJaZu3TBM"
   },
   "source": [
    "Con una red con cerca de 100000 parámetros se llega a una taza de error de menos del $6\\%$ en la clasificación.\n",
    "\n",
    "Hasta aquí hemos visto que:\n",
    "\n",
    "![ML comici](https://imgs.xkcd.com/comics/machine_learning.png)\n",
    "\n",
    "Fuente [xkcd](https://xkcd.com/1838/)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TP SofiaV2.ipynb",
   "provenance": [
    {
     "file_id": "1tKZzFEuIKSPEaeKJSsjcoSDtL_hAYVTe",
     "timestamp": 1563133211114
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
