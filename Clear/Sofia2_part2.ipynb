{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sofia2-part2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "845P6usPK5V5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy\n",
        "\n",
        "#Parche porque las versiones de Numpy y Keras son parcialemente incompatibles\n",
        "#Numpy cambió allow_pickle a False y eso hace que falle.\n",
        "#En la nueva versión de keras el problema está solucionado, pero no viene por defecto en colab.\n",
        "old_load = numpy.load\n",
        "def load(file, mmap_mode=None, allow_pickle=True, fix_imports=True, encoding='ASCII'):\n",
        "  return old_load(file, mmap_mode, allow_pickle, fix_imports, encoding)\n",
        "\n",
        "numpy.load = load"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98dgeXjyOGgX",
        "colab_type": "text"
      },
      "source": [
        "# Trabajando con lenguaje natural\n",
        "\n",
        "El lenguaje natural presenta muchos desafios a la hora de ser procesados por técnicas de Machine Learning, incluidas las de Deep Learning. En partícular, las características más importantes son:\n",
        "\n",
        "* Si bien existen muchas palabras, un texto generalmente utiliza solo una pequeña cantidad de ellas.\n",
        "* El orden de las palabras hacen al sentido del texto, las mismas palabras en distinto orden pueden tener distintos significados.\n",
        "* El largo del texto no es fijo, es decir distintos textos pueden tener distinta cantidad de palabras.\n",
        "\n",
        "Tradicionalmente, una de las formas de representar texto para machine learning es utilizar \"bag-of-words\", o alguna variación que cuente la frecuencia de las palabras en el texto y corpus, como [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), ver documentación de [sk-learn](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction). Estas técnicas representan el texto como un vector donde cada elemento es una palabra del vocabulario, y su valor representa el peso de esa palabra en el texto.\n",
        "\n",
        "$$ Vector=(pal_1, pal_2, ..., pal_m) $$\n",
        "\n",
        "Por ejemplo, una representación con pesado binario, es decir, el elemento del vector vale 1 si la palabra está en el texto y 0 sino. Si asumimos un vocabulario de 100 palabras y el texto \"Hola, mi nombre es Pedro\", la representación del texto tendrá cinco unos y 95 ceros.  \n",
        "\n",
        "## Large Movie Review Dataset\n",
        "\n",
        "[Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/) es un dataset de reviewes de películas extraido de IMDB. El objetivo es determinar si el review es positivo o negativo.\n",
        "\n",
        "Keras lo provee preprocesado, donde todas las palabras han sido asignadas a un número."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIpeFvGSTrSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import chain\n",
        "from keras.datasets import imdb\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data()\n",
        "\n",
        "print('Instancias de entrenamiento: {}'.format(len(x_train)))\n",
        "print('Ejemplo de una instancia: {}'.format(x_train[0]))\n",
        "\n",
        "words = set(chain.from_iterable(x_train))\n",
        "print('Palabras en el vocabulario: {}'.format(len(words)))\n",
        "\n",
        "overlap = [len(set(x))/len(words) * 100 for x in x_train]\n",
        "print('En promedio, una review utiliza {} palabras'.format(numpy.average([len(set(x)) for x in x_train])))\n",
        "print('En promedio, una review tiene el {}% del vocabulario'.format(numpy.average(overlap)))\n",
        "print('La review que menos palabras tiene utiliza el {}% del vocabulario'.format(numpy.min(overlap)))\n",
        "print('La review que más palabras tiene utiliza el {}% del vocabulario'.format(numpy.max(overlap)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61Q1mdxFk8K-",
        "colab_type": "text"
      },
      "source": [
        "## Ejemplo clásico\n",
        "\n",
        "Utilizando una representación binaria, podemos implementar una regresión lógistica para clasificar si un review es positivo o no.\n",
        "\n",
        "Debido a que keras tiene limitado soporte de matrices sparse, los requerimientos de memoria hacen difícil procesar todo el dataset ya que se requieren 25000 * 88585 * 32 bits de memoria (8 Gigas aproximadamente). Por este motivo, no procesaremos todo el dataset sino que utilizaremos un generador. Un generador permite procesar datos a medida que se necesitan. \n",
        "\n",
        "A continuación se muestra un ejemplo de un generador para la serie de Fibonacci. Cada vez que se ejecuta la instrucción yield, la función queda suspendida retornando el valor indicado. Cuando se pide un siguiente valor, la función continúa ejecutando desde el punto que se detuvo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAEojUV1rCMX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fib(x):\n",
        "    a, b = 1, 1\n",
        "    i = 1\n",
        "    while i <= x:\n",
        "        yield a\n",
        "        a, b = b, a + b\n",
        "        i += 1\n",
        "    return\n",
        "\n",
        "for i in fib(10):\n",
        "    print(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEZP22w_aGt6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data()\n",
        "\n",
        "#Mapeando palabras\n",
        "words = list(words)\n",
        "words.sort()\n",
        "words = {w: i for i, w in enumerate(words)}\n",
        "\n",
        "\n",
        "#Generador infinito que cicla por todos los elementos\n",
        "def generator(x, y, batch=128):\n",
        "    #Llevo un contador de posicion\n",
        "    i = 0\n",
        "    while(True):\n",
        "        #Genero los x e y del batch actual\n",
        "        res = numpy.zeros((batch, len(words)))\n",
        "        res_i = numpy.zeros((batch,))\n",
        "        #Cuento cuantos elementos voy generando\n",
        "        count = 0\n",
        "        while(True):\n",
        "            #Genero la representación para la review i\n",
        "            s = x[i]\n",
        "            for w in set(s):\n",
        "                if w in words:\n",
        "                    res[count, words[w]] = 1\n",
        "            res_i[count] = y[i]\n",
        "            #Incremento i y el countador de batch en 1. Si i es mayor que \n",
        "            #la cantidad de elementos, retorno a zero\n",
        "            i = (i + 1) % len(x)\n",
        "            count += 1\n",
        "            #si llegue a la cantidad maxima \n",
        "            #hago el yield del estado actual\n",
        "            if count == batch:\n",
        "                yield res, res_i\n",
        "                break\n",
        "    return \n",
        " \n",
        "print('Representación de las dos primeras instancias de entrenamiento')\n",
        "x, y = next(generator(x_train, y_train))\n",
        "print(x[0, :].tolist())\n",
        "print(x[1, :].tolist())\n",
        "\n",
        "#Regresión logística\n",
        "i = Input((len(words),))\n",
        "o = Dense(1, activation='sigmoid')(i)\n",
        "\n",
        "model = Model(inputs=i, outputs=o)\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#Entrenando con generador\n",
        "print('Entrenando...')\n",
        "model.fit_generator(generator(x_train, y_train),\n",
        "          steps_per_epoch=int(len(x_train)/128 + 1),\n",
        "          epochs=1)\n",
        "print('Evaluando...')\n",
        "score, acc = model.evaluate_generator(generator(x_test, y_test),\n",
        "          steps=int(len(x_test)/128 + 1))\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NpPIb-lbkM_",
        "colab_type": "text"
      },
      "source": [
        "## Word Embeddings\n",
        "\n",
        "Word embeddings es una técnica donde a cada palabra se le asigna un vector de características. Este vector tiene por objetivo representar la semantica de la palabra. Al utilizar esta técnica, un texto queda representado por una matriz de tamaño cantidad de palabras por cantidad de características.\n",
        "\n",
        "Ejemplo, supongamos un lenguaje con 10 palabras, y un texto que contenga las palabras ```[1, 3, 5, 5] ```. Si utilizamos un embedding de 20 características, el texto quedará representado como una matriz de 4x20.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOaOQShocuHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Input, Embedding\n",
        "from keras.models import Model\n",
        "\n",
        "text = [1, 3, 5, 5]\n",
        "\n",
        "print('El texto es {}'.format(text))\n",
        "#Le agrego una dimensión más que representa todas las instancias\n",
        "#ya que Keras espera intancias x características\n",
        "x_input = numpy.asarray([text]) \n",
        "\n",
        "#None indica que no se cuantas palabras tiene el texto\n",
        "i = Input((None,)) \n",
        "o = Embedding(10, 20)(i)\n",
        "#Este modelo retorna la matriz que representa al texto\n",
        "model = Model(inputs=i, outputs=o)\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print('Ejemplo de como se representa el texto')\n",
        "print(model.predict(x_input)[0, :, :])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmeg86-nyWNz",
        "colab_type": "text"
      },
      "source": [
        "Obviamente, el embedding está sin entrenar, por lo que el resultado es de poca utilidad. En la práctica se puede utilizar modelos preentrenados como los disponibles para: \n",
        "\n",
        "* Word2Vec\n",
        "* FastText\n",
        "* GloVe\n",
        "\n",
        "O entrenar el embedding ad-hoc. En nuestro caso entrenaremos el embedding.\n",
        "\n",
        "Una técnica muy popular para clasificar texto, es utilizar CNN. En vez de utilizar las 2D como en el caso de las imagenes, se utiliza las CNN 1D que consideran número de palabras X caractéristicas. \n",
        "\n",
        "Para facilitar el entrenamiento:\n",
        "\n",
        "* Haremos un padding del texto para que todos tengan el mismo largo. El padding es un proceso que ajusta la longitud de las oraciones. Si la oración es más larga que el maxímo largo, solo considera los últimos elementos. Si la oración es más corta, añade ceros al principio.\n",
        "* El elemento 0 está reservado, por lo que el indice de palabras comienza en 1.\n",
        "\n",
        "Una vez pasado por la CNN1D, utilizaremos una capa GlobalMaxPooling, que para cada uno de los filtros se queda con el máximo valor. Finalmente utilizaremos una capa densa para la predicción. Además, utilizaremos capas dropout que arbitrariamente cambian valores de los resultados por ceros durante el entrenamiento. Estas capas se utilizan para mejorar la calidad del entrenamiento ya que introducen ruido.\n",
        "\n",
        "Basado en los ejemplos presentados en el [Github de Keras](https://github.com/keras-team/keras)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmFz47Feg0xl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Dropout, Input\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "from keras.datasets import imdb\n",
        "\n",
        "# set parameters:\n",
        "max_features = 88585\n",
        "maxlen = 400\n",
        "batch_size = 32\n",
        "embedding_dims = 50\n",
        "filters = 250\n",
        "kernel_size = 3\n",
        "hidden_dims = 250\n",
        "epochs = 5\n",
        "\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "print('Pad sequences (ejemplos x largo)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "print('Creando modelo...')\n",
        "i = Input((maxlen,))\n",
        "o = Embedding(max_features, embedding_dims)(i)\n",
        "o = Dropout(0.2)(o)\n",
        "o = Conv1D(filters,\n",
        "                 kernel_size,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1)(o)\n",
        "o = GlobalMaxPooling1D()(o)\n",
        "o = Dense(hidden_dims, activation='relu')(o)\n",
        "o = Dropout(0.2)(o)\n",
        "o = Dense(1, activation='sigmoid')(o)\n",
        "\n",
        "model = Model(inputs=i, outputs=o)\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "score, acc = model.evaluate(x_test, y_test)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6-t_mJ9kG14",
        "colab_type": "text"
      },
      "source": [
        "## Redes LSTM\n",
        "\n",
        "Otra opción es utilizar redes de tipo recurrentes, como las LSTM. Estas redes suelen ser utilizadas en texto ya que consideran el orden de los elementos y propagan el calculo de manera secuencias. Esto en principio es una \"ventaja\" con respecto a las convolucionales que solo consideran localidad.\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg\" alt=\"Shallow NN\" style=\"width: 400px;\"/>\n",
        "\n",
        "> Red Neuronal Recurrente. Imagen: [Wikipedia](https://en.wikipedia.org/wiki/Recurrent_neural_network) <br>\n",
        "\n",
        "En particular, las LSTM tiene una estructura que no solo consideran la salida de la última capa, sino que además el estado interno.\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3b/The_LSTM_cell.png/1920px-The_LSTM_cell.png\"/>\n",
        "\n",
        ">Capa LSTM del artículo [LSTM de Wipipedia](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3b/The_LSTM_cell.png/1920px-The_LSTM_cell.png)\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f_t &= \\sigma_g(W_{f} x_t + U_{f} h_{t-1} + b_f) \\\\\n",
        "i_t &= \\sigma_g(W_{i} x_t + U_{i} h_{t-1} + b_i) \\\\\n",
        "o_t &= \\sigma_g(W_{o} x_t + U_{o} h_{t-1} + b_o) \\\\\n",
        "c_t &= f_t \\circ c_{t-1} + i_t \\circ \\sigma_c(W_{c} x_t + U_{c} h_{t-1} + b_c) \\\\\n",
        "h_t &= o_t \\circ \\sigma_h(c_t)\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6pMdSJtKRe1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Embedding, Input\n",
        "from keras.layers import LSTM\n",
        "from keras.datasets import imdb\n",
        "\n",
        "max_features = 88585\n",
        "# cut texts after this number of words (among top max_features most common words)\n",
        "maxlen = 80\n",
        "batch_size = 32\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "print('Pad sequences (ejemplos x largo)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "print('Creando modelo...')\n",
        "\n",
        "i = Input((maxlen,))\n",
        "o = Embedding(max_features, embedding_dims)(i)\n",
        "o = LSTM(embedding_dims, dropout=0.2, recurrent_dropout=0.2)(o)\n",
        "o = Dense(1, activation='sigmoid')(o)\n",
        "\n",
        "model = Model(inputs=i, outputs=o)\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=1, \n",
        "          validation_data=(x_test, y_test))\n",
        "score, acc = model.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuD8LOraqUy2",
        "colab_type": "text"
      },
      "source": [
        "## Consideraciones\n",
        "\n",
        "1. Las CNN son más rápidas pero no consideran el orden de las palabras, solo su localidad.\n",
        "1. Las LSTM consideran el orden, pero cuando las secuencias son largas tiende a no darle la misma importancia al comienzo de la oración. \n",
        "1. Las LSTM son lentas de entrenar ya que tienen un componente secuencial que no puede aprovechar el soporte de computación paralela ofrecida por las GPUs y TPUs.\n",
        "1. El ejemplo es muy sencillo y no se puede apreciar las ventajas de un modelo sobre el otro. \n",
        "\n",
        "\n",
        "## Otro ejemplo de LSTM: Generación de texto\n",
        "\n",
        "En este caso, el objetivo es generar texto que \"parezca\" ingles. Para esto, utilizaremos textos de Nietzsche en ingles. Para esto:\n",
        "\n",
        "1. Dado una secuencia de carácteres (40 en este ejemplo), cual es el siguiente. Esto puede verse como clasificación, donde la clase es el siguiente carácter.\n",
        "1. Los carácteres se representan utilizando one-hot. Es decir, un vector de largo cantidad de carácteres, donde todos los valores son cero, salvo el elemento que representa al carácter que es uno.\n",
        "1. Para la generación se utiliza muestreo y no la clase de mayor probabilidad (esta última opción no funciona bien).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtFVWugjsFmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.utils.data_utils import get_file\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "\n",
        "path = get_file(\n",
        "    'nietzsche.txt',\n",
        "    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
        "with io.open(path, encoding='utf-8') as f:\n",
        "    text = f.read().lower()\n",
        "print('corpus length:', len(text))\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "print('total chars:', len(chars))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# cut the text in semi-redundant sequences of maxlen characters\n",
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print('nb sequences:', len(sentences))\n",
        "\n",
        "print('Vectorization...')\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n",
        "\n",
        "print('Ejemplo de la representación:')\n",
        "print('Primera oración: ', sentences[0])\n",
        "print('Siguente carácter: ')\n",
        "print(next_chars[0])\n",
        "print('Mapa de indices: ', char_indices)\n",
        "print('Representación oración: ')\n",
        "print('[')\n",
        "for s in x[0, :, :].astype(numpy.int8).tolist():\n",
        "    print(s)\n",
        "print(']')\n",
        "print('Representación clase: ', y[0, :].astype(numpy.int8))\n",
        "\n",
        "# build the model: a single LSTM\n",
        "print('Construyendo modelo...')\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "optimizer = RMSprop(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "\n",
        "#Sample aplicando una temperatura que hace que el resultado\n",
        "#sea más o menos aleatoreo\n",
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    for diversity in [0.2, 0.5, 1]:\n",
        "        print('----- diversity:', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        #Texto random para iniciar la predicción\n",
        "        sentence = text[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "        #Predice 100 carácteres\n",
        "        for i in range(100):\n",
        "            #Fromatea la entrada a la red\n",
        "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.\n",
        "            #Predice las probabilidades del siguiente carácter y samplea\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "            #Elimina el primer carácter de la sentencia raiz y agrega el\n",
        "            #carácter predicho al final\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            #Imprime el resultado.\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "print('Generando sin entrenamiento')\n",
        "on_epoch_end(0, None)\n",
        "\n",
        "print('Entrenando y generando')\n",
        "model.fit(x, y,\n",
        "          batch_size=128,\n",
        "          epochs=2,\n",
        "          callbacks=[print_callback])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Lsl7Ia78B6T",
        "colab_type": "text"
      },
      "source": [
        "## Sunspring \n",
        "\n",
        "Utilizaron está técnica para generar [Sunspring](https://www.youtube.com/watch?v=LY7x2Ihqjmc), un corto de SciFi cuyo libreto fue generado por una red neuronal.\n",
        "\n",
        "# Autoencoders\n",
        "\n",
        "Un autoencoder son dos redes neuronales, una cuyo objetivo es reducir la representación de las instancias a un espacio dimensional más bajo. Por ejemplo, en el ejemplo de la MNIST existen 784 caractéristicas y se podría desear reducirlas a un número menor. Esto se llama reducción de dimensionalidad y tiene diversas aplicaciones, como entrenar modelos que no se comportan bien cuando hay muchas caractéristicas, eliminar caractéristicas redundantes, o reducir el nivel de ruido. Un ejemplo de utilización de autoencoders puede ser para comprimir imagenes, de hecho se ha probado que son competitivos cuando se comparan con estandares de la industria como JPEG2000 [1]. La arquitectura de un autoencoder es:\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/28/Autoencoder_structure.png\"/>\n",
        "\n",
        "> [Autoencoder](https://en.wikipedia.org/wiki/Autoencoder)\n",
        "\n",
        "\n",
        "\n",
        "[1] Theis, L., Shi, W., Cunningham, A., & Huszár, F. (2017). Lossy image compression with compressive autoencoders. arXiv preprint arXiv:1703.00395.\n",
        "\n",
        "\n",
        "## Ejemplo de autoencoder.\n",
        "\n",
        "El siguiente ejemplo, basado en los [ejemplos de Keras](https://github.com/keras-team/keras/blob/master/examples/mnist_denoising_autoencoder.py), utilizaremos un autoencoder para sacar ruido del MNIST. En el caso del ejemplo, se agregará ruido artificialmente. En particular a cada pixel se le agregará un ruido de media 0.5 y desviación estandard de 0.5. Notese que los pixeles están normalizados a valores entre 0 y 1, por lo que el ruido es significativo.\n",
        "\n",
        "El encoder tiene las siguiente arquitectura:\n",
        "\n",
        "1. Entrada de 28 x 28 x 1\n",
        "1.  Convolucional de 32 filtros y kernel de 3x3\n",
        "1.  Convolucional de 64 filtros y kernel de 3x3\n",
        "1. Capa de aplanado. Cada imagen resulta en vectores de 3136 elementos\n",
        "1. Densa con 16 neuronas\n",
        "\n",
        "\n",
        "Es decir, al final del encoder cada imagen queda representada por un vector de 16 caractéristicas en lugar de 784 pixeles.\n",
        "\n",
        "El decoder, quien es el encargado de regenerar la imagen tiene la siguiente arquitectura:\n",
        "\n",
        "1. Entrada de 16\n",
        "1. Una capa densa con 3136 salidas\n",
        "1. Deconvolución de 64 filtros\n",
        "1. Deconvolución de 32 filtros\n",
        "1. Deconvolución de 1 filtro. Reconstruendo la imagen original.\n",
        "\n",
        "\n",
        "Las deconvoluciones son operaciones que permiten reconstruir imagenes a las que se le aplicaron filtros convolucionales. Ver: [Deconvolutional Networks](https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2M_teJx9-Yd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import keras\n",
        "from keras.layers import Activation, Dense, Input\n",
        "from keras.layers import Conv2D, Flatten\n",
        "from keras.layers import Reshape, Conv2DTranspose\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "np.random.seed(1337)\n",
        "\n",
        "# MNIST dataset\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "image_size = x_train.shape[1]\n",
        "x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n",
        "x_test = np.reshape(x_test, [-1, image_size, image_size, 1])\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# Generate corrupted MNIST images by adding noise with normal dist\n",
        "# centered at 0.5 and std=0.5\n",
        "noise = np.random.normal(loc=0.5, scale=0.5, size=x_train.shape)\n",
        "x_train_noisy = x_train + noise\n",
        "noise = np.random.normal(loc=0.5, scale=0.5, size=x_test.shape)\n",
        "x_test_noisy = x_test + noise\n",
        "\n",
        "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
        "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
        "\n",
        "# Network parameters\n",
        "input_shape = (image_size, image_size, 1)\n",
        "batch_size = 128\n",
        "kernel_size = 3\n",
        "latent_dim = 16\n",
        "# Encoder/Decoder number of CNN layers and filters per layer\n",
        "layer_filters = [32, 64]\n",
        "\n",
        "# Build the Autoencoder Model\n",
        "# First build the Encoder Model\n",
        "inputs = Input(shape=input_shape, name='encoder_input')\n",
        "x = inputs\n",
        "# Stack of Conv2D blocks\n",
        "# Notes:\n",
        "# 1) Use Batch Normalization before ReLU on deep networks\n",
        "# 2) Use MaxPooling2D as alternative to strides>1\n",
        "# - faster but not as good as strides>1\n",
        "for filters in layer_filters:\n",
        "    x = Conv2D(filters=filters,\n",
        "               kernel_size=kernel_size,\n",
        "               strides=2,\n",
        "               activation='relu',\n",
        "               padding='same')(x)\n",
        "\n",
        "# Shape info needed to build Decoder Model\n",
        "shape = K.int_shape(x)\n",
        "\n",
        "# Generate the latent vector\n",
        "x = Flatten()(x)\n",
        "latent = Dense(latent_dim, name='latent_vector')(x)\n",
        "\n",
        "# Instantiate Encoder Model\n",
        "encoder = Model(inputs, latent, name='encoder')\n",
        "print('Encoder')\n",
        "encoder.summary()\n",
        "\n",
        "# Build the Decoder Model\n",
        "latent_inputs = Input(shape=(latent_dim,), name='decoder_input')\n",
        "x = Dense(shape[1] * shape[2] * shape[3])(latent_inputs)\n",
        "x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
        "\n",
        "# Stack of Transposed Conv2D blocks\n",
        "# Notes:\n",
        "# 1) Use Batch Normalization before ReLU on deep networks\n",
        "# 2) Use UpSampling2D as alternative to strides>1\n",
        "# - faster but not as good as strides>1\n",
        "for filters in layer_filters[::-1]:\n",
        "    x = Conv2DTranspose(filters=filters,\n",
        "                        kernel_size=kernel_size,\n",
        "                        strides=2,\n",
        "                        activation='relu',\n",
        "                        padding='same')(x)\n",
        "\n",
        "x = Conv2DTranspose(filters=1,\n",
        "                    kernel_size=kernel_size,\n",
        "                    padding='same')(x)\n",
        "\n",
        "outputs = Activation('sigmoid', name='decoder_output')(x)\n",
        "\n",
        "# Instantiate Decoder Model\n",
        "decoder = Model(latent_inputs, outputs, name='decoder')\n",
        "print('Decoder')\n",
        "decoder.summary()\n",
        "\n",
        "# Autoencoder = Encoder + Decoder\n",
        "# Instantiate Autoencoder Model\n",
        "print('Encoder-decoder apliado para entrenamiento')\n",
        "autoencoder = Model(inputs, decoder(encoder(inputs)), name='autoencoder')\n",
        "autoencoder.summary()\n",
        "\n",
        "autoencoder.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "# Train the autoencoder\n",
        "autoencoder.fit(x_train_noisy,\n",
        "                x_train,\n",
        "                validation_data=(x_test_noisy, x_test),\n",
        "                epochs=30,\n",
        "                batch_size=batch_size)\n",
        "\n",
        "# Predict the Autoencoder output from corrupted test images\n",
        "x_decoded = autoencoder.predict(x_test_noisy)\n",
        "\n",
        "# Display the 1st 8 corrupted and denoised images\n",
        "rows, cols = 10, 30\n",
        "num = rows * cols\n",
        "imgs = np.concatenate([x_test[:num], x_test_noisy[:num], x_decoded[:num]])\n",
        "imgs = imgs.reshape((rows * 3, cols, image_size, image_size))\n",
        "imgs = np.vstack(np.split(imgs, rows, axis=1))\n",
        "imgs = imgs.reshape((rows * 3, -1, image_size, image_size))\n",
        "imgs = np.vstack([np.hstack(i) for i in imgs])\n",
        "imgs = (imgs * 255).astype(np.uint8)\n",
        "plt.rcParams['figure.figsize'] = [25, 25]\n",
        "plt.figure()\n",
        "plt.axis('off')\n",
        "plt.title('Original images: top rows, '\n",
        "          'Corrupted Input: middle rows, '\n",
        "          'Denoised Input:  third rows')\n",
        "plt.imshow(imgs, interpolation='none', cmap='gray')\n",
        "Image.fromarray(imgs).save('corrupted_and_denoised.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt8ERm1n_MTk",
        "colab_type": "text"
      },
      "source": [
        "## Visualizando los resultado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFqozteF_MsH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.rcParams['figure.figsize'] = [25, 25]\n",
        "plt.figure()\n",
        "plt.axis('off')\n",
        "plt.title('Original images: top rows, '\n",
        "          'Corrupted Input: middle rows, '\n",
        "          'Denoised Input:  third rows')\n",
        "plt.imshow(imgs, interpolation='none', cmap='gray')\n",
        "Image.fromarray(imgs).save('corrupted_and_denoised.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVbEGXWGCXBq",
        "colab_type": "text"
      },
      "source": [
        "## Variational autoencoder\n",
        "\n",
        "Los variational autoencoders son considerados modelos generativos, es decir, que se pueden construir nuevas intancias utilizando este modelo. Al igual que el autoencoder presentado anteriormente, reduce la representación a un espacio dimensional menor. Sin embargo, este espacio representan poblaciones de distribución normal representado por su media y varianza (generalmente se utiliza el logaritmo de la varianza). Por este motivo, a la hora de reconstruir el decoder deber realizar un sampling de este espacio normal. Al ser un sampling, la reconstrucción no es deterministica por lo que se generan nuevas instancias similares a las anteriores.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNwziyhqCXhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "from keras.layers import Lambda, Input, Dense\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist\n",
        "from keras.losses import mse, binary_crossentropy\n",
        "from keras.utils import plot_model\n",
        "from keras import backend as K\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "\n",
        "# reparameterization trick\n",
        "# instead of sampling from Q(z|X), sample epsilon = N(0,I)\n",
        "# z = z_mean + sqrt(var) * epsilon\n",
        "def sampling(args):\n",
        "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
        "    # Arguments\n",
        "        args (tensor): mean and log of variance of Q(z|X)\n",
        "    # Returns\n",
        "        z (tensor): sampled latent vector\n",
        "    \"\"\"\n",
        "\n",
        "    z_mean, z_log_var = args\n",
        "    batch = K.shape(z_mean)[0]\n",
        "    dim = K.int_shape(z_mean)[1]\n",
        "    # by default, random_normal has mean = 0 and std = 1.0\n",
        "    epsilon = K.random_normal(shape=(batch, dim))\n",
        "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "\n",
        "def plot_results(models,\n",
        "                 data,\n",
        "                 batch_size=128,\n",
        "                 model_name=\"vae_mnist\"):\n",
        "    \"\"\"Plots labels and MNIST digits as a function of the 2D latent vector\n",
        "    # Arguments\n",
        "        models (tuple): encoder and decoder models\n",
        "        data (tuple): test data and label\n",
        "        batch_size (int): prediction batch size\n",
        "        model_name (string): which model is using this function\n",
        "    \"\"\"\n",
        "\n",
        "    encoder, decoder = models\n",
        "    x_test, y_test = data\n",
        "    os.makedirs(model_name, exist_ok=True)\n",
        "\n",
        "    filename = os.path.join(model_name, \"vae_mean.png\")\n",
        "    # display a 2D plot of the digit classes in the latent space\n",
        "    z_mean, z_log_var, _ = encoder.predict(x_test,\n",
        "                                   batch_size=batch_size)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.scatter(z_log_var[:, 0], z_log_var[:, 1], c=y_test)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "    \n",
        "    filename = os.path.join(model_name, \"digits_over_latent.png\")\n",
        "    # display a 30x30 2D manifold of digits\n",
        "    n = 30\n",
        "    digit_size = 28\n",
        "    figure = np.zeros((digit_size * n, digit_size * n))\n",
        "    # linearly spaced coordinates corresponding to the 2D plot\n",
        "    # of digit classes in the latent space\n",
        "    grid_x = np.linspace(-4, 4, n)\n",
        "    grid_y = np.linspace(-4, 4, n)[::-1]\n",
        "\n",
        "    for i, yi in enumerate(grid_y):\n",
        "        for j, xi in enumerate(grid_x):\n",
        "            z_sample = np.array([[xi, yi]])\n",
        "            x_decoded = decoder.predict(z_sample)\n",
        "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "            figure[i * digit_size: (i + 1) * digit_size,\n",
        "                   j * digit_size: (j + 1) * digit_size] = digit\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    start_range = digit_size // 2\n",
        "    end_range = (n - 1) * digit_size + start_range + 1\n",
        "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
        "    sample_range_x = np.round(grid_x, 1)\n",
        "    sample_range_y = np.round(grid_y, 1)\n",
        "    plt.xticks(pixel_range, sample_range_x)\n",
        "    plt.yticks(pixel_range, sample_range_y)\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.imshow(figure, cmap='Greys_r')\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "image_size = x_train.shape[1]\n",
        "original_dim = image_size * image_size\n",
        "x_train = np.reshape(x_train, [-1, original_dim])\n",
        "x_test = np.reshape(x_test, [-1, original_dim])\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# network parameters\n",
        "input_shape = (original_dim, )\n",
        "intermediate_dim = 512\n",
        "batch_size = 128\n",
        "latent_dim = 2\n",
        "epochs = 50\n",
        "\n",
        "# VAE model = encoder + decoder\n",
        "# build encoder model\n",
        "inputs = Input(shape=input_shape, name='encoder_input')\n",
        "x = Dense(intermediate_dim, activation='relu')(inputs)\n",
        "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
        "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
        "\n",
        "# use reparameterization trick to push the sampling out as input\n",
        "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
        "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
        "\n",
        "# instantiate encoder model\n",
        "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "encoder.summary()\n",
        "plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n",
        "\n",
        "# build decoder model\n",
        "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
        "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
        "outputs = Dense(original_dim, activation='sigmoid')(x)\n",
        "\n",
        "# instantiate decoder model\n",
        "decoder = Model(latent_inputs, outputs, name='decoder')\n",
        "decoder.summary()\n",
        "plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n",
        "\n",
        "# instantiate VAE model\n",
        "outputs = decoder(encoder(inputs)[2])\n",
        "vae = Model(inputs, outputs, name='vae_mlp')\n",
        "\n",
        "def run():\n",
        "    models = (encoder, decoder)\n",
        "    data = (x_test, y_test)\n",
        "\n",
        "    # VAE loss = mse_loss or xent_loss + kl_loss\n",
        "    #reconstruction_loss = mse(inputs, outputs)\n",
        "    reconstruction_loss = binary_crossentropy(inputs, outputs)\n",
        "\n",
        "    reconstruction_loss *= original_dim\n",
        "    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "    kl_loss = K.sum(kl_loss, axis=-1)\n",
        "    kl_loss *= -0.5\n",
        "    vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
        "    vae.add_loss(vae_loss)\n",
        "    vae.compile(optimizer='adam')\n",
        "    vae.summary()\n",
        "    plot_model(vae,\n",
        "               to_file='vae_mlp.png',\n",
        "               show_shapes=True)\n",
        "    \n",
        "    # train the autoencoder\n",
        "    vae.fit(x_train,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_data=(x_test, None))\n",
        "    #vae.save_weights('vae_mlp_mnist.h5')\n",
        "\n",
        "    plot_results(models,\n",
        "                 data,\n",
        "                 batch_size=batch_size,\n",
        "                 model_name=\"vae_mlp\")\n",
        "    \n",
        "run()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}